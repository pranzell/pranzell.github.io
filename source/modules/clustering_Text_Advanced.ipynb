{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Text Clustering\n",
    "\n",
    "A modern approach to clustering textual data using 2-passes of K-Means Algorithm "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@author: Pranjal Pathak\n",
    "@date: 2023-15-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Total Read Time ~ 20 mins.`\n",
    "\n",
    "`Total Execution Time ~ 45 mins.`\n",
    "\n",
    "#### Clustering\n",
    "1. Imports\n",
    "2. Directory Setup\n",
    "3. Load Data\n",
    "4. Preprocessing\n",
    "5. Set Configuration\n",
    "6. Vectorization\n",
    "7. Find Near-Duplicates using pair-wise similarity\n",
    "8. Clustering Dataset\n",
    "9. Clustering\n",
    "10. Final Output\n",
    "11. Metrics and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c76b52dce74ae1a19a13a3b48b6fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n",
      "Spacy loaded.\n",
      "PyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "'''Python 3.8.0'''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "from ast import literal_eval\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/somepath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "#from spacy.lang.en import English\n",
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as Functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt, ticker as ticker\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_language_detection.spacy_language_detector.LanguageDetector at 0x7fd753428bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.abspath(\".\")\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# Generic NLP resources\n",
    "nlp_resources_fp = \"../../_Resources_/nlp_resources/\"\n",
    "\n",
    "# Sentence Bert\n",
    "sbert_model_fp = \"../../_Resources_/transformer_models/all-distilroberta-v1/\"\n",
    "\n",
    "# Spacy\n",
    "spacy_model_data_path = \"../../_Resources_/spacy/en_core_web_lg-3.5.0/en_core_web_lg/en_core_web_lg-3.5.0\"\n",
    "nlp = spacy.load(spacy_model_data_path)  # disabling: nlp = spacy.load(spacy_data_path, disable=['ner'])\n",
    "def create_lang_detector(nlp, name): \n",
    "    return LanguageDetector()\n",
    "Language.factory(\"language_detector\", func=create_lang_detector)\n",
    "nlp.max_length = 2000000\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3394, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All campus dining locations are closed today, ...</td>\n",
       "      <td>cu_others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#FPGA Design for #Embedded #Systems\\n\\n#SoC #V...</td>\n",
       "      <td>cu_online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As an anthro PhD student I‚Äôm frequently asked ...</td>\n",
       "      <td>cu_research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True or False: Concussions for college student...</td>\n",
       "      <td>cu_research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@michaelgrandner @CUBoulder Very interesting w...</td>\n",
       "      <td>cu_research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@ShellyMBoulder Thanks so much @ShellyMBoulder...</td>\n",
       "      <td>cu_research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@LiberalsAreMean @CUBoulder Ha! In my day job,...</td>\n",
       "      <td>cu_research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Hutchison speaks to @5280Magazine about th...</td>\n",
       "      <td>cu_online</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet        label\n",
       "0  All campus dining locations are closed today, ...    cu_others\n",
       "1  What are your campus dining options today, Jan...    cu_others\n",
       "2  #FPGA Design for #Embedded #Systems\\n\\n#SoC #V...    cu_online\n",
       "3  What are your campus dining options today, Jan...    cu_others\n",
       "4  As an anthro PhD student I‚Äôm frequently asked ...  cu_research\n",
       "5  True or False: Concussions for college student...  cu_research\n",
       "6  @michaelgrandner @CUBoulder Very interesting w...  cu_research\n",
       "7  @ShellyMBoulder Thanks so much @ShellyMBoulder...  cu_research\n",
       "8  @LiberalsAreMean @CUBoulder Ha! In my day job,...  cu_research\n",
       "9  Dr. Hutchison speaks to @5280Magazine about th...    cu_online"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, \"tweets.csv\"))\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessText:\n",
    "    \n",
    "    def __init__(self, resources_dir_path, custom_vocab=[], MIN_TOKENS=5, MAX_TOKENS=1000000, do_lemma=False):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n",
    "        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n",
    "        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n",
    "        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n",
    "        self.preserve_key = \"<$>\" # preserve special vocab\n",
    "        self.vocab_list = custom_vocab\n",
    "        self.preseve = True if len(custom_vocab) > 0 else False\n",
    "        self.load_resources()\n",
    "        self.do_lemma = do_lemma\n",
    "        self.lang = []\n",
    "        self.min_tokens = MIN_TOKENS\n",
    "        self.max_tokens = MAX_TOKENS\n",
    "        return\n",
    "    \n",
    "    def load_resources(self):\n",
    "        \n",
    "        ### Build Vocab Model --> Words to keep\n",
    "        self.vocab_list = set(map(str.lower, self.vocab_list))\n",
    "        self.vocab_dict = {w: self.preserve_key.join(w.split()) for w in self.vocab_list}\n",
    "        self.re_retain_words = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n",
    "        \n",
    "        ### Build Stopwords Model --> Words to drop/delete\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        self.stopwords = list(sorted(set(self.stopwords).difference(self.vocab_list)))\n",
    "\n",
    "        ### Build Contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "        \n",
    "        ### Build Chat-words\n",
    "        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.chat_words_map_dict, self.chat_words_list = {}, []\n",
    "            chat_words = [x.rstrip() for x in f.readlines()]\n",
    "            for line in chat_words:\n",
    "                cw = line.split(\"=\")[0]\n",
    "                cw_expanded = line.split(\"=\")[1]\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "            self.chat_words_list = set(self.chat_words_list)\n",
    "        \n",
    "        ### Bukd social markups\n",
    "        \n",
    "        # Emoticons\n",
    "        with open(self.emoticons_file, \"r\") as f:\n",
    "            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n",
    "        \n",
    "        # Emojis\n",
    "        self.emojis = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        # Greeting\n",
    "        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.greeting_words = [x.rstrip() for x in f.readlines()]\n",
    "        \n",
    "        # Signature\n",
    "        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.signature_words = [x.rstrip() for x in f.readlines()]\n",
    "        \n",
    "        # Spell-corrector (takes too long!)\n",
    "        self.spell_checker = SpellChecker()   \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def reserve_keywords_from_cleaning(self, text, reset=False):\n",
    "        \"\"\" \n",
    "        Finds common words from a user-provided list of special keywords to preserve them from \n",
    "        cleaning steps. Identifies every special keyword and joins them using `self.preserve_key` during the \n",
    "        cleaning steps, and later resets it back to original word in the end.\n",
    "        \"\"\"\n",
    "        if reset is False:\n",
    "            # compile using a dict of words and their expansions, and sub them if found!\n",
    "            match_and_sub = self.re_retain_words.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n",
    "        else:\n",
    "            # reverse the change! - use this at the end of preprocessing\n",
    "            text = text.replace(self.preserve_key, \" \")\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    def basic_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in tqdm.tqdm(input_sentences):\n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <‚Ä¶> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0‚Äì9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def deep_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in tqdm.tqdm(input_sentences):\n",
    "            # normalize text to \"utf-8\" encoding\n",
    "            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            #\n",
    "            # *** Mark important keywords such as: Domain specific, Question words(wh-words), etc, using \n",
    "            # \"self.vocab_list\". Words from this list if found in any input sentence shall be joined using \n",
    "            # a key (self.preserve_key) during pre-processing step, and later un-joined to retain them.\n",
    "            #\n",
    "            # TWITTER SPECIFIC\n",
    "            # ----------------\n",
    "            twitter_anchor_re = \"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\"\n",
    "            sent = re.sub(r\"{}\".format(twitter_anchor_re), \" \", sent)\n",
    "            \n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=False)\n",
    "                \n",
    "            # Tokenizing with NLTK's TweetTokenizer (limiting repeated characters to 3 with the reduce lens\n",
    "            # paramater and strips all the @'s. It also splits it into 1-gram tokens\n",
    "            tweetToknzr = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "            sent = \" \".join(tweetToknzr.tokenize(sent))\n",
    "            \n",
    "            # Remove prefix of hastags (#) \n",
    "            sent = \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", sent).split())\n",
    "            #\n",
    "            #\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "\n",
    "            # remove Emojis üòú üî•üî•\n",
    "            sent = self.emojis.sub(r'', sent)\n",
    "            # remove Emoticons ( Õ°‚ùõ‚ÄØÕú ñ Õ°‚ùõ)\n",
    "            sent = self.emoticons.sub(r'', sent)\n",
    "            # remove common chat-words\n",
    "            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <‚Ä¶> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Removing web-links\n",
    "            sent = \" \".join([re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', token.strip(), flags=re.MULTILINE) for token in sent.split()])\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0‚Äì9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "\n",
    "            # sentence language detection ('en', 'fr', 'gr', etc)\n",
    "            lang = nlp(sent)._.language['language']\n",
    "            self.lang.append(lang)\n",
    "\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            #\n",
    "            # removing punctuations \n",
    "            # *** disable them, when sentence structure needs to be retained ***\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n",
    "            \n",
    "            # remove sentence de-limitters üî•üî•\n",
    "            # *** disable them, when sentence boundary/ending is important ***\n",
    "            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)\n",
    "\n",
    "            # keep only text & numbers üî•üî•\n",
    "            # *** enable them, when only text and numbers matter! *** \n",
    "            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            \n",
    "            # correct spelling mistakes üî•üî•\n",
    "            # *** TAKES TOO LONG! enable them when english spelling mistakes matter *** \n",
    "            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n",
    "            \n",
    "            # Limit Min - Max Token Length\n",
    "            sent = \"\" if len(sent.split()) > self.max_tokens or len(sent) <= self.min_tokens else sent\n",
    "            #\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "            # <--------------------------------------------------------------------------- >\n",
    "            \n",
    "            # Remove stopwords\n",
    "            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords and token.lemma_ not in self.stopwords)\n",
    "            # Lemmatize\n",
    "            if self.do_lemma:\n",
    "                sent = \" \".join(token.lemma_ for token in nlp(sent))\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).lower().strip()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Reverse the custom joining now to un-join the special words found!\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=True)\n",
    "            #\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            cleaned_sentences.append(sent.strip().lower())\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def spacy_get_pos_list(self, results):\n",
    "        word_list, pos_list, lemma_list, ner_list, start_end_list = [], [], [], [], []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                # (1). save tokens\n",
    "                word_list.append(token['word'])\n",
    "                # (2). save pos\n",
    "                pos_list.append(token['pos'])\n",
    "                # (3). save lemmas\n",
    "                lemma = token['lemma'].lower()\n",
    "                if lemma in self.stopwords: continue\n",
    "                lemma_list.append(lemma)\n",
    "                # (4). save NER\n",
    "                ner_list.append(token['ner'])\n",
    "                # (5). save start\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        output = {\"word_list\": word_list, \n",
    "                  \"lemma_list\": lemma_list, \n",
    "                  \"token_start_end_list\": start_end_list,\n",
    "                  \"pos_list\": pos_list, \"ner_list\": ner_list}\n",
    "        return output\n",
    "\n",
    "    def spacy_generate_features(self, doc, operations='tokenize,ssplit,pos,lemma,ner'):\n",
    "        \"\"\"\n",
    "        Spacy nlp pipeline to generate features such as pos, tokens, ner, dependency. Accepts doc=nlp(text)\n",
    "        \"\"\"\n",
    "        # spacy doc\n",
    "        doc_json = doc.to_json()  # Includes all operations given by spacy pipeline\n",
    "\n",
    "        # Get text\n",
    "        text = doc_json['text']\n",
    "\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        # 1. Extract Entity List\n",
    "        entity_list = doc_json[\"ents\"]\n",
    "\n",
    "        # 2. Create token lib\n",
    "        token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "        # init output json\n",
    "        output_json = {}\n",
    "        output_json[\"sentences\"] = []\n",
    "\n",
    "        # Perform spacy operations on each sent in text\n",
    "        for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "            # init parsers\n",
    "            parse = \"\"\n",
    "            basicDependencies = []\n",
    "            enhancedDependencies = []\n",
    "            enhancedPlusPlusDependencies = []\n",
    "\n",
    "            # init output json\n",
    "            out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "            output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "            # 3. Split sentences by indices(i), add labels (pos, ner, dep, etc.)\n",
    "            for token in doc_json[\"tokens\"]:\n",
    "\n",
    "                if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                    \n",
    "                    # >>> Extract Entity label\n",
    "                    ner = \"O\"\n",
    "                    for entity in entity_list:\n",
    "                        if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                            ner = entity[\"label\"]\n",
    "\n",
    "                    # >>> Extract dependency info\n",
    "                    dep = token[\"dep\"]\n",
    "                    governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                    governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]:\n",
    "                                                                                     token_lib[token[\"head\"]][\"end\"]]\n",
    "                    dependent = token[\"id\"] + 1\n",
    "                    dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "\n",
    "                    # >>> Extract lemma\n",
    "                    lemma = doc[token[\"id\"]].lemma_\n",
    "\n",
    "                    # 4. Add dependencies\n",
    "                    basicDependencies.append({\"dep\": dep,\n",
    "                                              \"governor\": governor,\n",
    "                                              \"governorGloss\": governorGloss,\n",
    "                                              \"dependent\": dependent,\n",
    "                                              \"dependentGloss\": dependentGloss})\n",
    "                    # 5. Add tokens\n",
    "                    out_token = {\"index\": token[\"id\"] + 1,\n",
    "                                 \"word\": dependentGloss,\n",
    "                                 \"originalText\": dependentGloss,\n",
    "                                 \"characterOffsetBegin\": token[\"start\"],\n",
    "                                 \"characterOffsetEnd\": token[\"end\"]}\n",
    "\n",
    "                    # 6. Add lemmas\n",
    "                    if \"lemma\" in operations:\n",
    "                        out_token[\"lemma\"] = lemma\n",
    "\n",
    "                    # 7. Add POS tagging\n",
    "                    if \"pos\" in operations:\n",
    "                        out_token[\"pos\"] = token[\"tag\"]\n",
    "\n",
    "                    # 8. Add NER\n",
    "                    if \"ner\" in operations:\n",
    "                        out_token[\"ner\"] = ner\n",
    "\n",
    "                    # Update output json\n",
    "                    out_sentence[\"tokens\"].append(out_token)\n",
    "\n",
    "            # 9. Add dependencies operation\n",
    "            if \"parse\" in operations:\n",
    "                out_sentence[\"parse\"] = parse\n",
    "                out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "                out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "                out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        return output_json\n",
    "    \n",
    "    def spacy_clean(self, input_sentences):\n",
    "        batch_size = min(int(np.ceil(len(input_sentences)/100)), 500)\n",
    "        \n",
    "        # Part 1: generate spacy textual features (pos, ner, lemma, dependencies)\n",
    "        sentences = [self.spacy_generate_features(doc) for doc in nlp.pipe(input_sentences, batch_size=batch_size, n_process=-1)]\n",
    "        \n",
    "        # Part 2: collect all the features for each sentence\n",
    "        spacy_sentences = [self.spacy_get_pos_list(sent) for sent in sentences]\n",
    "\n",
    "        return spacy_sentences\n",
    "\n",
    "\n",
    "    ## MAIN ##\n",
    "    def run_pipeline(self, data, text_col, operation=['deep']):\n",
    "        \"\"\"Main module to execute pipeline. Accepts list of strings, and desired operation.\"\"\"\n",
    "\n",
    "        if operation==\"\":\n",
    "            raise Exception(\"Please pass a cleaning type - `basic`, `deep` or `spacy` !!\")\n",
    "        if not isinstance(operation, list):\n",
    "            operation = [operation]\n",
    "        operation = list(map(str.lower, operation))\n",
    "\n",
    "        # RAW SENTENCES\n",
    "        sentences, lang_masker = data[text_col], \"\"\n",
    "\n",
    "        # run basic cleaning\n",
    "        if \"basic\" in operation:\n",
    "            sentences = self.basic_clean(sentences)\n",
    "\n",
    "        # run deep cleaning\n",
    "        if \"deep\" in operation:\n",
    "            sentences, lang_masker = self.deep_clean(sentences), self.lang\n",
    "\n",
    "        # run spacy pipeline\n",
    "        if \"spacy\" in operation:\n",
    "            sentences = self.spacy_clean(sentences)\n",
    "\n",
    "        data[\"Processed_%s\" % text_col], data['lang_mask'] = sentences, lang_masker\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "# :: SAMPLE ::\n",
    "# preprocessText_obj = preprocessText(nlp_resources_fp, custom_vocab, MIN_TOKENS, MAX_TOKENS, do_lemmatizing)\n",
    "# data_clean = preprocessText_obj.run_pipeline(dataset, <TEXT_COL>, [\"deep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Execute Preprocessing Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### :: configuration ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CUSTOM VOCABULARY ::\n",
    "- List of words you wish to mark and retain them across the preprocessing steps - very important!\n",
    "- Example, task-specific, domain-specific keywords. \n",
    "\"\"\"\n",
    "\n",
    "# Preserve: question words for FAQs.\n",
    "# custom_vocab = [\"who\", \"what\", \"where\", \"when\", \"would\", \"which\", \"how\", \"why\", \"can\", \"may\", \n",
    "#                 \"will\", \"won't\", \"does\", \"does not\",\"doesn't\", \"do\", \"do i\", \"do you\", \"is it\", \"would you\", \n",
    "#                 \"is there\", \"are there\", \"is it so\", \"is this true\", \"to know\", \"is that true\", \"are we\", \n",
    "#                 \"am i\", \"question is\", \"can i\", \"can we\", \"tell me\", \"can you explain\", \"how ain't\", \n",
    "#                 \"question\", \"answer\", \"questions\", \"answers\", \"ask\", \"can you tell\"]\n",
    "\n",
    "\n",
    "# Preserve: CUBoulder related words.\n",
    "custom_vocab = [\"sko\", \"buff\", \"buffs\", \"cu\", \"cub\", \"uni\", \"univer\", \"cubb\", \"ucb\", \"skobuff\", \"skobuffs\", \n",
    "                \"cuboulder\", \"colorado\", \"boulder\", \"footb\", \"ds\", \"onl\", \"res\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Utilities:\n",
    "- Truncate words to their root-known-word form, stripping off their adjectives, verbs, etc.\n",
    "\"\"\"\n",
    "\n",
    "MIN_TOKENS = 5\n",
    "MAX_TOKENS = 50\n",
    "do_lemmatizing = True\n",
    "# do_chinking = False\n",
    "# do_chunking = False\n",
    "# do_dependencyParser = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### :: run ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:18<00:00, 27.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned! Total time taken (seconds) =  18.079297065734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessText_obj = preprocessText(nlp_resources_fp, custom_vocab, MIN_TOKENS, MAX_TOKENS, do_lemmatizing)\n",
    "\n",
    "start_time = time.time()\n",
    "df_clean = preprocessText_obj.run_pipeline(df, \"tweet\", [\"deep\"])\n",
    "\n",
    "print(\"Cleaned! Total time taken (seconds) = \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final preprocessed data\n",
    "\n",
    "df = df_clean.dropna(subset=['Processed_tweet']).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>Processed_tweet</th>\n",
       "      <th>lang_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All campus dining locations are closed today, ...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>campus dining location close today jan 1 happy...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>what your campus dining option today jan 2 alf...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#FPGA Design for #Embedded #Systems\\n\\n#SoC #V...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>fpga design embed system soc verilog vlsi asic...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>what your campus dining option today jan 3 alf...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As an anthro PhD student I‚Äôm frequently asked ...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>anthro phd student m frequently ask why study ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True or False: Concussions for college student...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>true false concussion college student signific...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@michaelgrandner @CUBoulder Very interesting w...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>interesting work additional tool investigation...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@ShellyMBoulder Thanks so much @ShellyMBoulder...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>thank much fun talk research</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@LiberalsAreMean @CUBoulder Ha! In my day job,...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>ha day job turbulence model computational flui...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Hutchison speaks to @5280Magazine about th...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>dr hutchison speak cannabis health course cour...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet        label  \\\n",
       "0  All campus dining locations are closed today, ...    cu_others   \n",
       "1  What are your campus dining options today, Jan...    cu_others   \n",
       "2  #FPGA Design for #Embedded #Systems\\n\\n#SoC #V...    cu_online   \n",
       "3  What are your campus dining options today, Jan...    cu_others   \n",
       "4  As an anthro PhD student I‚Äôm frequently asked ...  cu_research   \n",
       "5  True or False: Concussions for college student...  cu_research   \n",
       "6  @michaelgrandner @CUBoulder Very interesting w...  cu_research   \n",
       "7  @ShellyMBoulder Thanks so much @ShellyMBoulder...  cu_research   \n",
       "8  @LiberalsAreMean @CUBoulder Ha! In my day job,...  cu_research   \n",
       "9  Dr. Hutchison speaks to @5280Magazine about th...    cu_online   \n",
       "\n",
       "                                     Processed_tweet lang_mask  \n",
       "0  campus dining location close today jan 1 happy...        en  \n",
       "1  what your campus dining option today jan 2 alf...        en  \n",
       "2  fpga design embed system soc verilog vlsi asic...        en  \n",
       "3  what your campus dining option today jan 3 alf...        en  \n",
       "4  anthro phd student m frequently ask why study ...        en  \n",
       "5  true false concussion college student signific...        en  \n",
       "6  interesting work additional tool investigation...        en  \n",
       "7                       thank much fun talk research        en  \n",
       "8  ha day job turbulence model computational flui...        en  \n",
       "9  dr hutchison speak cannabis health course cour...        en  "
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Configuration Parameters\n",
    "\n",
    "Ideally, should store in os.environ or configParser object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ::  config  ::    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these\n",
    "colname_txt = \"tweet\"\n",
    "colname_clean_txt = \"Processed_tweet\"\n",
    "\n",
    "# these will be created run-time\n",
    "colname_id = \"UID\"\n",
    "colname_dup_id_col = \"dup_idx\"\n",
    "colname_similar_id_col = \"similar_idx\"\n",
    "colname_dup_similar_id_col = \"dup_similar_idx\"\n",
    "colname_duplicate_cluster_id = \"dup_cluster_id\"\n",
    "colname_freq = \"memberCount\"\n",
    "colname_coverage = \"coverage\"\n",
    "colname_cluster_id = \"cluster_id\"\n",
    "kmeans_k = 0\n",
    "kmeans_rate = 5\n",
    "kmeans_seedInit = 50\n",
    "kmeans_maxIter = 1000\n",
    "kmeans_clusterLen = 20\n",
    "kmeans_cohesion_threshold = 0.70\n",
    "kmeans_len2Level = 100\n",
    "kmeans_lenThreshold = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(data):\n",
    "    config = dict()\n",
    "    # FIRST, create unique id for each text record\n",
    "    data.insert(0, colname_id, ['uidx_%s' % i for i in range(len(data))]) \n",
    "    config['colname_id'] = colname_id\n",
    "    config['colname_txt'] = colname_txt\n",
    "    config['colname_clean_txt'] = colname_clean_txt\n",
    "    config['colname_id'] = colname_id\n",
    "    config['colname_dup_id_col'] = colname_dup_id_col\n",
    "    config['colname_similar_id_col'] = colname_similar_id_col\n",
    "    config['colname_dup_similar_id_col'] = colname_dup_similar_id_col\n",
    "    config['colname_duplicate_cluster_id'] = colname_duplicate_cluster_id\n",
    "    config['colname_freq'] = colname_freq\n",
    "    config['colname_coverage'] = colname_coverage\n",
    "    config['colname_cluster_id'] = colname_cluster_id\n",
    "    config['kmeans_k'] = kmeans_k\n",
    "    config['kmeans_rate'] = kmeans_rate\n",
    "    config['kmeans_seedInit'] = kmeans_seedInit\n",
    "    config['kmeans_maxIter'] = kmeans_maxIter\n",
    "    config['kmeans_clusterLen'] = kmeans_clusterLen\n",
    "    config['kmeans_cohesion_threshold'] = kmeans_cohesion_threshold\n",
    "    config['kmeans_len2Level'] = kmeans_len2Level\n",
    "    config['kmeans_lenThreshold'] = kmeans_lenThreshold\n",
    "    config['nlp_resources_fp'] = nlp_resources_fp\n",
    "    config['spacy_model_data_path'] = spacy_model_data_path\n",
    "    return config\n",
    "\n",
    "\n",
    "config = create_config(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context:\n",
    "\n",
    "\n",
    "- An $embedding$ is a numerical representation of words, phrases, or sentences that captures the meaning and context (both semantic and syntactic) of the text in a high-dimensional space. \n",
    "\n",
    "\n",
    "- In the case of a document (one or more sentences), an $embedding$ represents the meaning of these sentences in a way that can be processed by machine learning models.\n",
    "\n",
    "\n",
    "- A $Transformer$ model is a type of neural network that is commonly used for natural language processing. One of the main features of the Transformer model is its ability to generate word embeddings that capture the meaning and context of a sentence. These embeddings are generated by processing the sentence through the layers of the Transformer model.\n",
    "\n",
    "\n",
    "Our data:\n",
    "\n",
    "- In our dataset (tweets) each sentence have been vectorized into a vector of [1 x 768] dimensions.\n",
    "\n",
    "\n",
    "- When we say that a text sentence is embedded into $768 \\space dimensions$ by a pre-trained Transformer model, we mean that the sentence is **now represented as a vector of 768 numbers**, where each number in the vector corresponds to a different dimension in the embedding space. \n",
    "\n",
    "\n",
    "- These dimensions were learned by the Transformer model during training and represent different aspects of the meaning and context of the sentence.\n",
    "\n",
    "\n",
    "- For example, one dimension of the embedding vector might correspond to the presence of a certain topic or concept in the sentence, while another dimension might correspond to the sentiment or emotional tone of the sentence. By combining these different dimensions in a high-dimensional space, the Transformer model is able to capture the complex and nuanced meaning of a sentence.\n",
    "\n",
    "\n",
    "- In *extremely* simple words, \n",
    "\n",
    "            text      = \"Coach Prime era has begun!\"\n",
    "                        \n",
    "                        go  now coach prime have has  era  begin  end ....]\n",
    "        word_vector   = [0  0   1     1     0    1    1    1      0   ....]     => One-Hot-Encoding\n",
    "        \n",
    "        topics        =   start  end   sentiment   complexity  presence_of_Adjective_followed_by_a_Noun ...\n",
    "        \n",
    "        embedding     =  [0.38   0      0.56         -0.32       0.87      0   ]\n",
    "        \n",
    "                      = a meaningful vector representing meaning & context of the sentence in 768 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# List of Vectorizer Structures \n",
    "# choose - [ 'tfidf', 'bert', 'word2vec', 'doc2vec' ]\n",
    "################################################################################################\n",
    "\n",
    "class Bert_Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer, model, max_length=128, embedding_func: Optional[Callable[[torch.Tensor], torch.Tensor]]=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Mean Pooling - Take attention mask into account for correct averaging\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        encoded_input = self.tokenizer(text, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def transform(self, text: List[str], formatt='tensor'):\n",
    "        \"\"\" MODIFIED LATEST JAN 27, 2023\"\"\"\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "\n",
    "        # default previously returned embeddings\n",
    "        embeddings = self._tokenize(text)\n",
    "\n",
    "        # new **modified**\n",
    "        if formatt:\n",
    "            formatt = str(formatt).strip().lower()\n",
    "            if formatt == 'tensor':\n",
    "                return embeddings\n",
    "            elif formatt == 'numpy':\n",
    "                return embeddings.numpy()\n",
    "            elif formatt == 'csr':\n",
    "                embeddings_matrix = Functional.normalize(embeddings, p=2, dim=1)\n",
    "                embeddings_matrix_csr = csr_matrix(embeddings_matrix.numpy().astype(np.float64))\n",
    "                return embeddings_matrix_csr\n",
    "            else:\n",
    "                raise Exception(\"Invalid input for formatt!\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting (fine-tuning) is required\"\"\"\n",
    "        return self\n",
    "\n",
    "class Tfidf_Vectorizer():\n",
    "    def __init__(self, ngram_size=3):\n",
    "        self.ngram_size = ngram_size\n",
    "        self.regex = r'[,-./]|\\s'\n",
    "\n",
    "    def get_n_grams(self, string):\n",
    "        if string is not None: \n",
    "            string = string.lower()\n",
    "        string = re.sub(self.regex, r'', string)\n",
    "        n_grams = zip(*[string[i:] for i in range(self.ngram_size)])\n",
    "        return [''.join(n_gram) for n_gram in n_grams]\n",
    "    \n",
    "    def fit(self, txt):\n",
    "        self.vec = TfidfVectorizer(min_df=1, analyzer=self.get_n_grams, dtype=np.float64)\n",
    "        return self.vec.fit(txt)\n",
    "\n",
    "    def transform(self, txt):\n",
    "        return self.vec.transform(txt)\n",
    "\n",
    "\n",
    "class Word2vec_Vecotrizer:\n",
    "    def __init__(self):\n",
    "        print(\"Define a word2vec model here along with fit, transform operations.\")\n",
    "\n",
    "\n",
    "class Doc2vec_Vecotrizer:\n",
    "    def __init__(self):\n",
    "        print(\"Define a Doc2vec model here along with fit, transform operations.\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these after training/using vectorizer models:\n",
    "\n",
    "config['vectorizer_model_fp'] = {'tfidf': None, \n",
    "                                 'bert': sbert_model_fp, \n",
    "                                 'word2vec': None, \n",
    "                                 'doc2vec': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Computing Duplicates and Near-Duplicates\n",
    "\n",
    "One aspect of textual clustering is to aviod unwanted data-redundancy, which means, since we have already preprocessed our sentences (stripped of various adulterations and common words), multiple sentences might now be reduced to a very similar generic form and can be counted as duplicates or near-duplicates. \n",
    "\n",
    "- Performing clustering on such sparse matrix [n documents x 768] dimensions which might have near duplicates can significantly affect memory and space requirements. Hence we drop the near-duplicates processed tweets in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-part process.\n",
    "1. Part 1 involves computing pair-wise sentence-sentence similarity.\n",
    "2. Part 2 involves ranking similar sentences and consolidating them together as duplicates or near-duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Computing pair-wise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Pair-Wise Similarity Computation Structure \n",
    "################################################################################################\n",
    "class Generate_Similarity_Matrix(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 master, \n",
    "                 master_id=None, \n",
    "                 duplicates=None, \n",
    "                 duplicates_id=None, \n",
    "                 min_similarity=0.80, \n",
    "                 do_vectorize=True, \n",
    "                 vectorizer=None, \n",
    "                 vectorizer_pre_model_fp=None, \n",
    "                 vectorized_master_fp=None, \n",
    "                 vectorized_duplicates_fp=None):\n",
    "\n",
    "        # UTILITY FUNCTIONS\n",
    "        def _is_series_of_strings(series_to_test: pd.Series):\n",
    "            if not isinstance(series_to_test, pd.Series):\n",
    "                return False\n",
    "            elif series_to_test.to_frame().applymap(lambda x: not isinstance(x, str)).squeeze(axis=1).any():\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        def _is_input_data_combination_valid(duplicates, master_id, duplicates_id):\n",
    "            if duplicates is None and (duplicates_id is not None) or duplicates is not None and (\n",
    "                    (master_id is None) ^ (duplicates_id is None)):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "        # VALIDATE INPUT ARGS\n",
    "        if not _is_series_of_strings(master) or (duplicates is not None and not _is_series_of_strings(duplicates)):\n",
    "            raise TypeError('Input does not consist of pandas.Series containing only Strings')\n",
    "        if not _is_input_data_combination_valid(duplicates, master_id, duplicates_id):\n",
    "            raise Exception('List of data Series options is invalid')\n",
    "        if master_id is not None and len(master) != len(master_id):\n",
    "            raise Exception('Both master and master_id must be pandas.Series of the same length.')\n",
    "        if duplicates is not None and duplicates_id is not None and len(duplicates) != len(duplicates_id):\n",
    "            raise Exception('Both duplicates and duplicates_id must be pandas.Series of the same length.')\n",
    "        if do_vectorize and vectorizer is None:\n",
    "            raise Exception(\"Define a vectorizer engine using 'vectorizer=' first!\")\n",
    "        if do_vectorize and vectorizer.lower().strip() not in ['tfidf', 'word2vec', 'doc2vec', 'bert']:\n",
    "            raise Exception(\"Use a vectorizer from available list of engines defined!\")\n",
    "        if do_vectorize and vectorizer.lower() == 'bert' and vectorizer_pre_model_fp is None:\n",
    "            raise Exception(\"Using Transformers Model, define sBert fp using 'vectorizer_pre_model_fp=' first!\")\n",
    "        if do_vectorize is False and vectorized_master_fp is None:\n",
    "            raise Exception(\"Provide path of master_file vectorized pickle obj, using 'vectorized_master_fp='\")\n",
    "        if do_vectorize is False and duplicates is not None and vectorized_duplicates_fp is None:\n",
    "            raise Exception(\"Provide path of dup_file vectorized pickle obj, using 'vectorized_duplicates_fp='\")\n",
    "\n",
    "        # SAVE INPUT ARGS\n",
    "        self._master = master\n",
    "        self._duplicates = duplicates if duplicates is not None else None\n",
    "        self._master_id = master_id if master_id is not None else None\n",
    "        self._duplicates_id = duplicates_id if duplicates_id is not None else None\n",
    "        self.min_similarity = min_similarity\n",
    "        self.do_vectorize = do_vectorize\n",
    "        self.vectorizer_name = vectorizer.lower().strip() if vectorizer else None\n",
    "        self.vectorizer_pre_model_fp = vectorizer_pre_model_fp\n",
    "        self.master_vector_fp = vectorized_master_fp\n",
    "        self.duplicates_vector_fp = vectorized_duplicates_fp\n",
    "\n",
    "        # CONFIG\n",
    "        self._true_max_n_matches = None\n",
    "        self._max_n_matches = len(self._master) if self._duplicates is None else len(self._duplicates)\n",
    "        self.number_of_processes = multiprocessing.cpu_count() - 1\n",
    "        self.DEFAULT_COLUMN_NAME = 'side'\n",
    "        self.DEFAULT_ID_NAME = 'id'\n",
    "        self.LEFT_PREFIX = 'left_'\n",
    "        self.RIGHT_PREFIX = 'right_'\n",
    "        self._matches_list = pd.DataFrame()\n",
    "        self.is_build = False  # indicates if fit has been called or not\n",
    "\n",
    "\n",
    "        # ----------------------------------------------------------------------------- #\n",
    "        # INIT VECTORIZER ENGINE\n",
    "        # ----------------------------------------------------------------------------- #\n",
    "        ##########################################\n",
    "        # Option 1: Vectorize during run-time!\n",
    "        ##########################################\n",
    "        if self.do_vectorize:\n",
    "            if self.vectorizer_name == \"tfidf\":\n",
    "                self.ngram_size = 3\n",
    "                self._vectorizer = Tfidf_Vectorizer(self.ngram_size)\n",
    "            if self.vectorizer_name == \"word2vec\":\n",
    "                raise Exception(\"Word2vec is not yet defined! Define first in 'Word2vec_Vecotrizer' !\")\n",
    "            if self.vectorizer_name == \"doc2vec\":\n",
    "                raise Exception(\"Doc2vec is not yet defined! Define first in 'Doc2vec_Vecotrizer' !\")\n",
    "            if self.vectorizer_name == \"bert\":\n",
    "                tokenizer = AutoTokenizer.from_pretrained(self.vectorizer_pre_model_fp)\n",
    "                model_bert = AutoModel.from_pretrained(self.vectorizer_pre_model_fp)\n",
    "                self._vectorizer = Bert_Vectorizer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n",
    "\n",
    "        ##########################################\n",
    "        # Option 2: Load vectorized file!\n",
    "        ##########################################\n",
    "        else:\n",
    "            # load master file's vectorized object\n",
    "            with open(self.master_vector_fp, 'rb') as f:  \n",
    "                self.master_matrix_loaded_embed = pickle.load(f)\n",
    "            # load dup file's vectorized object\n",
    "            if self._duplicates is not None:              \n",
    "                with open(self.duplicates_vector_fp, 'rb') as f:\n",
    "                    self.duplicate_matrix_loaded_embed = pickle.load(f)\n",
    "            else:\n",
    "                # IF there is no duplicate matrix, match on the master matrix itself!\n",
    "                self.duplicate_matrix_loaded_embed = self.master_matrix_loaded_embed\n",
    "        # ----------------------------------------------------------------------------- #\n",
    "        return\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fit a vectorizer (already init) with Master & Duplicates matrix and calculate cosine-sim without original-ids.\n",
    "        Params  : Master, Duplicates\n",
    "        Return  : dataframe{ Master_Text, Duplicates_Text, cosine_sim(vectorizer_master, vectorized_duplicates) }\n",
    "\n",
    "        \"\"\"\n",
    "        # UTILITY FUNCTIONS\n",
    "        def fix_diagonal(m: lil_matrix):\n",
    "            r = np.arange(m.shape[0])\n",
    "            m[r, r] = 1\n",
    "            return m\n",
    "\n",
    "        def symmetrize_matrix(m_symmetric: lil_matrix):\n",
    "            r, c = m_symmetric.nonzero()\n",
    "            m_symmetric[c, r] = m_symmetric[r, c]\n",
    "            return m_symmetric\n",
    "\n",
    "        # Vectorize the matrices\n",
    "        # - if duplicate matrix is present use it, else utilize master itself\n",
    "        master_matrix, duplicate_matrix = self.get_vectorized_matrices()\n",
    "\n",
    "        # Calculate cosine similarity b/w master & duplicates (if passed, else use master itself)\n",
    "        matches = self.build_matches(master_matrix, duplicate_matrix)\n",
    "        self._true_max_n_matches = self._max_n_matches - 1\n",
    "\n",
    "        # Correct sparse matrix multiplcation\n",
    "        if self._duplicates is None:\n",
    "            # convert to lil format for best efficiency when setting matrix-elements\n",
    "            # matrix diagonal elements must be exactly 1 (numerical precision errors introduced by floating-point computations\n",
    "            #                                             in awesome_cossim_topn sometimes lead to unexpected results)\n",
    "            matches = matches.tolil()\n",
    "            matches = fix_diagonal(matches)\n",
    "            if self._max_n_matches < self._true_max_n_matches:\n",
    "                matches = symmetrize_matrix(matches)\n",
    "            matches = matches.tocsr()\n",
    "\n",
    "        # Create the basic \"matches\" dataframe with \"Master, Duplicate and Similarity\" cols only\n",
    "        r, c = matches.nonzero()\n",
    "        self._matches_list = pd.DataFrame(\n",
    "            {'master_side': r.astype(np.int64), 'dupe_side': c.astype(np.int64), 'similarity': matches.data})\n",
    "        self.is_build = True\n",
    "        return self\n",
    "\n",
    "    def get_vectorized_matrices(self):\n",
    "        \"\"\"\n",
    "        Vectorize matrices using one of the vectorizers or load vectorized text.\n",
    "        Params    : Master, Duplicates, Vectorizer_name(\"tfidf\", \"bert\", \"word2vec\", \"doc2vec\")\n",
    "        Return    : vectorizer_master, vectorized_duplicates\n",
    "        \"\"\"\n",
    "        def check_csr(matrix):\n",
    "            \"\"\"\n",
    "            Ideally every vectorized matrix should be in a CSR format for faster computations.\n",
    "            \"\"\"\n",
    "            if isinstance(matrix, np.ndarray):      # numpy format -> CSR\n",
    "                return csr_matrix(Functional.normalize(torch.from_numpy(matrix), p=2, dim=1).numpy().astype(np.float64))\n",
    "            elif torch.is_tensor(matrix):            # pyTorch.tensor format -> CSR\n",
    "                return csr_matrix(Functional.normalize(matrix, p=2, dim=1).numpy().astype(np.float64))\n",
    "            elif isinstance(matrix, csr_matrix):     # CSR format\n",
    "                return matrix\n",
    "            else:\n",
    "                raise Exception(\"Vectorized matrix format not identitifed! Check CSR properties.\")\n",
    "\n",
    "        def fit_vectorizer():\n",
    "            # if both master & duplicates series are set - concat them to fit the vectorizer on all strings at once!\n",
    "            if self._duplicates is not None:\n",
    "                strings = pd.concat([self._master, self._duplicates])\n",
    "            else:\n",
    "                strings = self._master\n",
    "            self._vectorizer.fit(strings)\n",
    "            return self._vectorizer\n",
    "\n",
    "        if self.do_vectorize:\n",
    "            \"\"\"\n",
    "            Vectorization during run-time!\n",
    "            \"\"\"\n",
    "            if self.vectorizer_name == \"tfidf\":\n",
    "                print(\"Vectorizing: tfidf\")\n",
    "                self._vectorizer = fit_vectorizer()\n",
    "                master_matrix = self._vectorizer.transform(self._master)\n",
    "                if self._duplicates is not None:\n",
    "                    duplicate_matrix = self._vectorizer.transform(self._duplicates)\n",
    "                else:\n",
    "                    # IF there is no duplicate matrix, match on the master matrix itself!\n",
    "                    duplicate_matrix = master_matrix\n",
    "\n",
    "            if self.vectorizer_name == \"bert\":\n",
    "                print(\"Vectorizing: bert\")\n",
    "                master_matrix = check_csr(self._vectorizer.transform(self._master, formatt='csr'))\n",
    "                if self._duplicates is not None:\n",
    "                    duplicate_matrix = check_csr(self._vectorizer.transform(self._duplicates, formatt='csr'))\n",
    "                else:\n",
    "                    # IF there is no duplicate matrix, match on the master matrix itself!\n",
    "                    duplicate_matrix = master_matrix\n",
    "                    \n",
    "            if self.vectorizer_name == \"word2vec\":\n",
    "                raise Exception(\"Word2vec is not yet defined! Define first in 'Word2vec_Vecotrizer' !\")\n",
    "            \n",
    "            if self.vectorizer_name == \"doc2vec\":\n",
    "                raise Exception(\"Doc2vec is not yet defined! Define first in 'Doc2vec_Vecotrizer' !\")     \n",
    "        else:\n",
    "            \"\"\"\n",
    "            Using prevectorized text.\n",
    "            \"\"\"\n",
    "            master_matrix = check_csr(self.master_matrix_loaded_embed)\n",
    "            duplicate_matrix = check_csr(self.duplicate_matrix_loaded_embed)\n",
    "\n",
    "        return master_matrix, duplicate_matrix\n",
    "\n",
    "    def build_matches(self, master_matrix, duplicate_matrix):\n",
    "        \"\"\"\n",
    "        Builds the cosine similarity matrix of two CSR matrices.\n",
    "        Params   : vectorizer_master, vectorized_duplicates\n",
    "        Return   : cosine_sim(vectorized_master, vectorized_duplicates)\n",
    "        \"\"\"\n",
    "        # Matrix A, B\n",
    "        tf_idf_matrix_1 = master_matrix\n",
    "        tf_idf_matrix_2 = duplicate_matrix.transpose()\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        optional_kwargs = {'use_threads': self.number_of_processes > 1, 'n_jobs': self.number_of_processes}\n",
    "        cosine_sim_matrix = awesome_cossim_topn(tf_idf_matrix_1,\n",
    "                                                tf_idf_matrix_2,\n",
    "                                                self._max_n_matches,\n",
    "                                                self.min_similarity,\n",
    "                                                **optional_kwargs)\n",
    "        return cosine_sim_matrix\n",
    "\n",
    "    def get_matches(self):\n",
    "        \"\"\"\n",
    "        Creates the complete dataframe with index matching(ids) if passed.\n",
    "        Params  : dataframe\n",
    "        Return  : dataframe{ Master_ids, Master_Text, cosine_similarity, Duplicate_ids, Duplicates_Text }\n",
    "        \"\"\"\n",
    "        # UTILITY FUNCTIONS\n",
    "        def get_both_sides(master, duplicates, generic_name=(self.DEFAULT_COLUMN_NAME, self.DEFAULT_COLUMN_NAME),\n",
    "                           drop_index=False):\n",
    "            lname, rname = generic_name\n",
    "            left = master if master.name else master.rename(lname)\n",
    "            left = left.iloc[matches_list.master_side].reset_index(drop=drop_index)\n",
    "            if self._duplicates is None:\n",
    "                right = master if master.name else master.rename(rname)\n",
    "            else:\n",
    "                right = duplicates if duplicates.name else duplicates.rename(rname)\n",
    "            right = right.iloc[matches_list.dupe_side].reset_index(drop=drop_index)\n",
    "            return left, (right if isinstance(right, pd.Series) else right[right.columns[::-1]])\n",
    "\n",
    "        def prefix_column_names(data, prefix):\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                return data.rename(columns={c: f\"{prefix}{c}\" for c in data.columns})\n",
    "            else:\n",
    "                return data.rename(f\"{prefix}{data.name}\")\n",
    "\n",
    "        if self.min_similarity > 0:\n",
    "            matches_list = self._matches_list\n",
    "        else:\n",
    "            raise Exception(\"min_similarity cannot be set to less than or equal to 0!\")\n",
    "\n",
    "        # ID Retrival\n",
    "        left_side, right_side = get_both_sides(self._master, self._duplicates, drop_index=False)\n",
    "        similarity = matches_list.similarity.reset_index(drop=True)\n",
    "        if self._master_id is None:\n",
    "            # if ids are not passed\n",
    "            return pd.concat([prefix_column_names(left_side, self.LEFT_PREFIX),\n",
    "                              similarity,\n",
    "                              prefix_column_names(right_side, self.RIGHT_PREFIX)], axis=1)\n",
    "\n",
    "        else:\n",
    "            # if ids are passed, retrive ids\n",
    "            left_side_id, right_side_id = get_both_sides(self._master_id, self._duplicates_id,\n",
    "                                                         (self.DEFAULT_ID_NAME, self.DEFAULT_ID_NAME), drop_index=True)\n",
    "            return pd.concat([prefix_column_names(left_side, self.LEFT_PREFIX),\n",
    "                              prefix_column_names(left_side_id, self.LEFT_PREFIX),\n",
    "                              similarity,\n",
    "                              prefix_column_names(right_side_id, self.RIGHT_PREFIX),\n",
    "                              prefix_column_names(right_side, self.RIGHT_PREFIX)], axis=1)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Compute pair-wise similarity.\n",
    "        \"\"\"\n",
    "        st = time.time()\n",
    "        self.fit()\n",
    "        sim_df = self.get_matches()\n",
    "        print(\"Total time taken (mins): \", (time.time() - st)/60)\n",
    "        return sim_df\n",
    "\n",
    "\n",
    "\n",
    "## :: SAMPLE RUN ::\n",
    "\n",
    "# Run mode 1: \n",
    "# Semantic Similarity among all strings of file_A using runtime vectorization:\n",
    "# matches = Generate_Similarity_Matrix(master = df[<_text_col_>], master_id = df[<_id_col_>], \n",
    "#                                      min_similarity=0.80, do_vectorize=True, vectorizer='bert|tfidf|word2vec')\n",
    "# matches.run()\n",
    "\n",
    "# Run mode 2: \n",
    "# Semantic Similarity among all strings of file_A using prevectorized text:\n",
    "# matches = Generate_Similarity_Matrix(master = df[<_text_col_>], master_id = df[<_id_col_>], \n",
    "#                                      min_similarity=0.80, do_vectorize=False, vectorized_master_fp=\"PATH\")\n",
    "# matches.run()\n",
    "\n",
    "# Run mode 3: \n",
    "# Semantic similarity between two files A and B using runtime vectorization:\n",
    "# matches = Generate_Similarity_Matrix(master = df[__text_col_A__], master_id = df[__id_col_A__], \n",
    "#                                      duplicates = df[__text_col_B__], duplicates_id = df[__id_col_B__], \n",
    "#                                      min_similarity=0.85, do_vectorize=True, vectorizer='bert|tfidf|word2vec')\n",
    "# matches.run()\n",
    "\n",
    "# Run mode 4: \n",
    "# Textual Semantic similarity between two files A and B using prevectorized text:\n",
    "# matches = Generate_Similarity_Matrix(master = df[__text_col_A__], master_id = df[__id_col_A__], \n",
    "#                                      duplicates = df[__text_col_B__], duplicates_id = df[__id_col_B__], \n",
    "#                                      min_similarity=0.85, do_vectorize=False, \n",
    "#                                      vectorized_master_fp='PATH', vectorized_duplicates_fp='PATH')\n",
    "# matches.run()\n",
    "\n",
    "# Run mode 5: \n",
    "# Textual Semantic similarity between two files A and B with no ids, using prevectorized text:\n",
    "# matches = Generate_Similarity_Matrix(master = df[__text_col_A__], duplicates = df[__text_col_B__]\n",
    "#                                      min_similarity=0.85, do_vectorize=False, \n",
    "#                                      vectorized_master_fp='PATH', vectorized_duplicates_fp='PATH')\n",
    "# matches.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Part 2: Finding duplicates and near-duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Use Pair-Wise Similarity to find near-duplicates\n",
    "################################################################################################\n",
    "class find_duplicates:\n",
    "    \n",
    "    def __init__(self, data, vectorizer, config, sim_threshold=0.90):\n",
    "        \n",
    "        self.org_data = self.data = data\n",
    "        self.sim_cutoff = sim_threshold\n",
    "        self.vectorizer = vectorizer.lower().strip()\n",
    "        if self.vectorizer not in ['tfidf', 'bert', 'word2vec', 'doc2vec']:\n",
    "            raise Exception(\"Invalid vectorizer passed!\")\n",
    "        \n",
    "        self.col_id = config.get('colname_id')\n",
    "        self.col_clean_txt = config.get('colname_clean_txt')\n",
    "        self.col_dup_id = config.get('colname_dup_id_col')\n",
    "        self.col_sim_id = config.get('colname_similar_id_col')\n",
    "        self.col_dup_sim_id = config.get('colname_dup_similar_id_col')\n",
    "        self.col_cluster_id = config.get('colname_duplicate_cluster_id')\n",
    "        self.vectorizer_model_fp = config['vectorizer_model_fp'][self.vectorizer]\n",
    "    \n",
    "    \n",
    "    def collect_dups(self):\n",
    "        \"\"\"\n",
    "        # 1. collect duplicate ids based on \"text\" col\n",
    "        \"\"\"\n",
    "        dup_dict = self.data.reset_index()\\\n",
    "                    .groupby(self.data[self.col_clean_txt].tolist())[self.col_id]\\\n",
    "                    .agg(list)\\\n",
    "                    .reset_index().reset_index(drop=True)\\\n",
    "                    .rename(columns={\"index\": self.col_clean_txt, self.col_id: self.col_dup_id})\n",
    "        dup_dict = dup_dict.set_index(self.col_clean_txt)[self.col_dup_id].to_dict()\n",
    "        self.data[self.col_dup_id] = self.data[self.col_clean_txt].apply(lambda txt: dup_dict[txt])\n",
    "        return\n",
    "   \n",
    "    def drop_dups(self):\n",
    "        \"\"\"\n",
    "        # 2. drop dup ids, keep first\n",
    "        \"\"\"\n",
    "        self.data = self.data.drop_duplicates(subset=[self.col_dup_id]).reset_index(drop=True)\n",
    "        return\n",
    "\n",
    "    def pairwise_similarity_matrix(self):\n",
    "        \"\"\"\n",
    "        # 3. collect pair-wise similairty matches above 'sim_threshold'\n",
    "        \"\"\"\n",
    "        print(\"Computing pair-wise similarity matrix above score: \", self.sim_cutoff)\n",
    "        # Pair-wise textual similarity\n",
    "        pwsim = Generate_Similarity_Matrix(master = self.data[self.col_clean_txt], \n",
    "                                           master_id = self.data[self.col_id], \n",
    "                                           min_similarity = self.sim_cutoff, \n",
    "                                           do_vectorize = True,\n",
    "                                           vectorizer = self.vectorizer,\n",
    "                                           vectorizer_pre_model_fp = self.vectorizer_model_fp)\n",
    "        matches = pwsim.run()\n",
    "\n",
    "        # group similar-pairs together (left-join)\n",
    "        left_col_name, left_unique_id, right_unique_id = \"left_%s\" % self.col_clean_txt, \"left_%s\" % self.col_id, \"right_%s\" % self.col_id\n",
    "        match_df = matches.groupby([left_col_name, left_unique_id])[right_unique_id]\\\n",
    "                          .agg(similar_idx = lambda x: sorted(set(x)))\\\n",
    "                          .reset_index()\\\n",
    "                          .sort_values(by=[left_unique_id], ascending=True)\\\n",
    "                          .reset_index(drop=True)\n",
    "\n",
    "        # asthestic: drop dummy added left/right names\n",
    "        matches = matches.drop(columns=['left_index', \"right_index\"])\n",
    "        match_df = match_df.rename(columns={left_unique_id: self.col_id, left_col_name: self.col_clean_txt})\n",
    "        return matches, match_df\n",
    "    \n",
    "    def natural_sort_key(self, s):\n",
    "        \"\"\"\n",
    "        ## Utility: alphanumeric sort\n",
    "        \"\"\"\n",
    "        _nsre = re.compile('([0-9]+)')\n",
    "        return [int(text) if text.isdigit() else text.lower() for text in re.split(_nsre, s)]\n",
    "        \n",
    "    def combine_dup_similar(self, match_df):\n",
    "        \"\"\"\n",
    "        # 4. create \"dup_similar_idx\" col - merge dup_id data with similar_id data\n",
    "        \"\"\"\n",
    "        # merge df_duplicates with df_similar\n",
    "        cols_to_use = [self.col_id, self.col_sim_id]\n",
    "        self.data = self.data.merge(match_df[cols_to_use], on=self.col_id, how='outer')\n",
    "        # create combined list == \"duplicated_pairs_idx\" + \"similar_pairs_idx\n",
    "        self.data[self.col_dup_sim_id] = [sorted(set(sum(tup, []))) for tup in zip(self.data[self.col_dup_id], self.data[self.col_sim_id])]\n",
    "        # custom sorting (to handle alphanumeric ids)\n",
    "        if isinstance(self.data[self.col_dup_sim_id][0], str):\n",
    "            self.data[self.col_dup_sim_id] = self.data[self.col_dup_sim_id].apply(lambda x: sorted(x, key=natural_sort_key))\n",
    "        return\n",
    "    \n",
    "    def collect_similar_ids(self):\n",
    "        \"\"\"\n",
    "        # 5. merged all nested lists containing common sub-elements in \"dup_similar_id\" cols\n",
    "        \"\"\"\n",
    "        # collect nested list which needs to be merged\n",
    "        list_similar_ids = list(map(list, self.data[self.col_dup_sim_id]))\n",
    "\n",
    "        # merge all nested lists with common elements\n",
    "        g = nx.Graph()\n",
    "        edges = [g.add_edges_from(zip(p, p[1:])) if len(p)>1 else g.add_edges_from(zip(p, p[:])) for p in list_similar_ids]\n",
    "        merged_similar_idx = [sorted(c) for c in nx.connected_components(g)]\n",
    "\n",
    "        # create two mappings, one for storing cluster_id: list of ids, and one inverted dict\n",
    "        # --> \"id_clus_dict\" is the cluster id mapping for each 'unique_id'\n",
    "        temp_id = 1\n",
    "        clus_id_dict = {}      # cluster_1: merged([id1, id2,..., idn])\n",
    "        id_clus_dict = {}      # merged(id1): cluster_1; merged(id1): cluster_1; .., merged(idn): cluster_1\n",
    "        for lst in merged_similar_idx:\n",
    "            key = \"dup_%s\" % temp_id\n",
    "            for value in lst:\n",
    "                id_clus_dict[value] = key\n",
    "            clus_id_dict[key] = lst\n",
    "            temp_id += 1 \n",
    "\n",
    "        # assign dup_similar_idx based on two mappings above\n",
    "        self.data[self.col_dup_sim_id] = self.data[self.col_id].apply(lambda uid: clus_id_dict[id_clus_dict[uid]])\n",
    "\n",
    "        # create duplicate id mapping and similar id mapping files\n",
    "        dup_id_dict = {_id: ids for ids in self.data[self.col_dup_id].tolist() for _id in ids}\n",
    "        sim_id_dict = {_id: ids for ids in self.data[self.col_sim_id].tolist() for _id in ids}\n",
    "        dup_sim_id_dict = {_id: ids for ids in self.data[self.col_dup_sim_id].tolist() for _id in ids}\n",
    "\n",
    "        # custom sorting (to handle alphanumeric ids)\n",
    "        if isinstance(self.data[self.col_dup_sim_id][0], str):\n",
    "            self.data[self.col_dup_sim_id] = self.data[self.col_dup_sim_id].apply(lambda x: sorted(x, key=natural_sort_key))\n",
    "        return clus_id_dict, id_clus_dict, dup_id_dict, sim_id_dict, dup_sim_id_dict\n",
    "    \n",
    "    def create_final_single_matrix(self):\n",
    "        \"\"\"\n",
    "        # 6. Drop duplicates based on dup_similar_id_col, i.e. duplicated_id + similar_ids\n",
    "        \"\"\"\n",
    "        self.data[self.col_dup_sim_id] = tuple(map(tuple, self.data[self.col_dup_sim_id]))\n",
    "        self.data = self.data.drop_duplicates(subset=[self.col_dup_sim_id]).reset_index(drop=True)\n",
    "        self.data[self.col_dup_sim_id] = list(map(list, self.data[self.col_dup_sim_id]))\n",
    "        return\n",
    "    \n",
    "    def create_clusters(self, idx_cluster_map):\n",
    "        \"\"\"\n",
    "        # 7. Expand each id to assign clusters\n",
    "        \"\"\"\n",
    "        self.data[self.col_cluster_id] = self.data[self.col_id].apply(lambda uid: idx_cluster_map.get(uid, -1))\n",
    "        return\n",
    "    \n",
    "    def make_changes_in_orig_df(self, dup_id_dict, sim_id_dict, dup_sim_id_dict, idx_cluster_map):\n",
    "        \"\"\"\n",
    "        # 8. Over-ride found details on the original dataset\n",
    "        \"\"\"\n",
    "        self.org_data[self.col_dup_id] = self.org_data[self.col_id].apply(lambda uid: dup_id_dict.get(uid, -1))\n",
    "        self.org_data[self.col_sim_id] = self.org_data[self.col_id].apply(lambda uid: sim_id_dict.get(uid, -1))\n",
    "        self.org_data[self.col_dup_sim_id] = self.org_data[self.col_id].apply(lambda uid: dup_sim_id_dict.get(uid, -1))\n",
    "        self.org_data[self.col_cluster_id] = self.org_data[self.col_id].apply(lambda uid: idx_cluster_map.get(uid, -1))\n",
    "        self.org_data[self.col_dup_sim_id] = tuple(map(tuple, self.org_data[self.col_dup_sim_id]))\n",
    "        print(\"> Duplicate and very similar records identified.\")\n",
    "        return\n",
    "\n",
    "    def create_cluster_data(self):\n",
    "        \"\"\"\n",
    "        # 9. Create final Clustering Dataset\n",
    "             - drop duplicates and very similar rows, they can be mapped back using 'dup_similar_idx'\n",
    "        \"\"\"\n",
    "        df_cluster = self.org_data.drop_duplicates(subset=[self.col_dup_sim_id]).reset_index(drop=True)\n",
    "        print('> Dups & very similar roecrds dropped. Final Unique df_cluster Shape = ', df_cluster.shape)\n",
    "        return df_cluster\n",
    "\n",
    "    def run_stats(self):\n",
    "        \"\"\"\n",
    "        # 10. Display Analytics\n",
    "        \"\"\"\n",
    "        print(\"Stats:\\n\"\n",
    "              \"\\nOrignal number of records = {}\"\n",
    "              \"\\nTotal dup count = {}\"\n",
    "              \"\\nTotal similar pairs found = {}\"\n",
    "              \"\\nFinal number of records post dup-similar rows removal = {}\"\\\n",
    "              .format(len(self.org_data), \n",
    "                      len(sum(self.data[self.col_dup_id].tolist(), [])), \n",
    "                      len(sum(self.data[self.col_sim_id].tolist(), [])),\n",
    "                      len(self.data)))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def EXE(self):\n",
    "        print(\"Original shape #records: \", self.org_data.shape[0])\n",
    "        \n",
    "        # collect dup rows\n",
    "        self.collect_dups()\n",
    "        \n",
    "        # drop dup rows\n",
    "        self.drop_dups()\n",
    "        print(\"Exact duplicates dropped. New shape #records: \", self.data.shape[0])\n",
    "\n",
    "        # compute pair-wise similarity for remanining rows\n",
    "        matches, match_df = self.pairwise_similarity_matrix()\n",
    "        \n",
    "        # merge dup_ids with very_similar_ids\n",
    "        self.combine_dup_similar(match_df)\n",
    "    \n",
    "        # merged nxn graphs for \"dup_similar_id\" cols\n",
    "        cluster_id_map, \\\n",
    "        idx_cluster_map, \\\n",
    "        dup_id_dict, \\\n",
    "        sim_id_dict, \\\n",
    "        dup_sim_id_dict = self.collect_similar_ids()\n",
    "        \n",
    "        # drop duplicates and near-duplicates based on dup_similar_id_col, i.e. duplicated_id + similar_ids\n",
    "        self.create_final_single_matrix()\n",
    "        \n",
    "        # expand each id to assign clusters\n",
    "        self.create_clusters(idx_cluster_map)\n",
    "        \n",
    "        # include changes in original data\n",
    "        self.make_changes_in_orig_df(dup_id_dict, sim_id_dict, dup_sim_id_dict, idx_cluster_map)\n",
    "        \n",
    "        # drop duplicate_similar_idx rows\n",
    "        df_cluster = self.create_cluster_data()\n",
    "            \n",
    "        # stats\n",
    "        self.run_stats()\n",
    "        \n",
    "        return self.org_data, df_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute: Two-part Near-Dup computation\n",
    "\n",
    "- Indetify duplicates and **very similar** records (with strict similarity around 90-95, else will contain noise), and collect their ids.\n",
    "- So that clustering can run on remaining **unique** records. We can later assign the same cluster_id to collected 'dup_similar_idx' ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape #records:  500\n",
      "Exact duplicates dropped. New shape #records:  469\n",
      "Computing pair-wise similarity matrix above score:  0.9\n",
      "Vectorizing: bert\n",
      "Total time taken (mins):  0.7107892870903015\n",
      "> Duplicate and very similar records identified.\n",
      "> Dups & very similar roecrds dropped. Final Unique df_cluster Shape =  (456, 9)\n",
      "Stats:\n",
      "\n",
      "Orignal number of records = 500\n",
      "Total dup count = 481\n",
      "Total similar pairs found = 469\n",
      "Final number of records post dup-similar rows removal = 456\n",
      "(500, 9) ---> (456, 9)\n"
     ]
    }
   ],
   "source": [
    "dup_finder = find_duplicates(df, 'bert', config, sim_threshold=0.90)\n",
    "df, df_cluster = dup_finder.EXE()\n",
    "print(df.shape, \"--->\", df_cluster.shape)\n",
    "\n",
    "# Save a copy!\n",
    "df.to_csv(os.path.join(data_dir, \"tweets_preprocessed.csv\"), index=False)\n",
    "df_cluster.to_csv(os.path.join(data_dir, \"tweets_cluster_final.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>Processed_tweet</th>\n",
       "      <th>lang_mask</th>\n",
       "      <th>dup_idx</th>\n",
       "      <th>similar_idx</th>\n",
       "      <th>dup_similar_idx</th>\n",
       "      <th>dup_cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uidx_0</td>\n",
       "      <td>All campus dining locations are closed today, ...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>campus dining location close today jan 1 happy...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_0]</td>\n",
       "      <td>[uidx_0]</td>\n",
       "      <td>(uidx_0,)</td>\n",
       "      <td>dup_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uidx_1</td>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>what your campus dining option today jan 2 alf...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_1]</td>\n",
       "      <td>[uidx_1, uidx_3]</td>\n",
       "      <td>(uidx_1, uidx_3)</td>\n",
       "      <td>dup_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uidx_2</td>\n",
       "      <td>#FPGA Design for #Embedded #Systems\\n\\n#SoC #V...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>fpga design embed system soc verilog vlsi asic...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_2]</td>\n",
       "      <td>[uidx_2]</td>\n",
       "      <td>(uidx_2,)</td>\n",
       "      <td>dup_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uidx_3</td>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>what your campus dining option today jan 3 alf...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_3]</td>\n",
       "      <td>[uidx_1, uidx_3]</td>\n",
       "      <td>(uidx_1, uidx_3)</td>\n",
       "      <td>dup_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uidx_4</td>\n",
       "      <td>As an anthro PhD student I‚Äôm frequently asked ...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>anthro phd student m frequently ask why study ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_4]</td>\n",
       "      <td>[uidx_4]</td>\n",
       "      <td>(uidx_4,)</td>\n",
       "      <td>dup_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uidx_5</td>\n",
       "      <td>True or False: Concussions for college student...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>true false concussion college student signific...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_5]</td>\n",
       "      <td>[uidx_5]</td>\n",
       "      <td>(uidx_5,)</td>\n",
       "      <td>dup_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uidx_6</td>\n",
       "      <td>@michaelgrandner @CUBoulder Very interesting w...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>interesting work additional tool investigation...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_6]</td>\n",
       "      <td>[uidx_6]</td>\n",
       "      <td>(uidx_6,)</td>\n",
       "      <td>dup_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uidx_7</td>\n",
       "      <td>@ShellyMBoulder Thanks so much @ShellyMBoulder...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>thank much fun talk research</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_7]</td>\n",
       "      <td>[uidx_7]</td>\n",
       "      <td>(uidx_7,)</td>\n",
       "      <td>dup_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uidx_8</td>\n",
       "      <td>@LiberalsAreMean @CUBoulder Ha! In my day job,...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>ha day job turbulence model computational flui...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_8]</td>\n",
       "      <td>[uidx_8]</td>\n",
       "      <td>(uidx_8,)</td>\n",
       "      <td>dup_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uidx_9</td>\n",
       "      <td>Dr. Hutchison speaks to @5280Magazine about th...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>dr hutchison speak cannabis health course cour...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_9]</td>\n",
       "      <td>[uidx_9]</td>\n",
       "      <td>(uidx_9,)</td>\n",
       "      <td>dup_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      UID                                              tweet        label  \\\n",
       "0  uidx_0  All campus dining locations are closed today, ...    cu_others   \n",
       "1  uidx_1  What are your campus dining options today, Jan...    cu_others   \n",
       "2  uidx_2  #FPGA Design for #Embedded #Systems\\n\\n#SoC #V...    cu_online   \n",
       "3  uidx_3  What are your campus dining options today, Jan...    cu_others   \n",
       "4  uidx_4  As an anthro PhD student I‚Äôm frequently asked ...  cu_research   \n",
       "5  uidx_5  True or False: Concussions for college student...  cu_research   \n",
       "6  uidx_6  @michaelgrandner @CUBoulder Very interesting w...  cu_research   \n",
       "7  uidx_7  @ShellyMBoulder Thanks so much @ShellyMBoulder...  cu_research   \n",
       "8  uidx_8  @LiberalsAreMean @CUBoulder Ha! In my day job,...  cu_research   \n",
       "9  uidx_9  Dr. Hutchison speaks to @5280Magazine about th...    cu_online   \n",
       "\n",
       "                                     Processed_tweet lang_mask   dup_idx  \\\n",
       "0  campus dining location close today jan 1 happy...        en  [uidx_0]   \n",
       "1  what your campus dining option today jan 2 alf...        en  [uidx_1]   \n",
       "2  fpga design embed system soc verilog vlsi asic...        en  [uidx_2]   \n",
       "3  what your campus dining option today jan 3 alf...        en  [uidx_3]   \n",
       "4  anthro phd student m frequently ask why study ...        en  [uidx_4]   \n",
       "5  true false concussion college student signific...        en  [uidx_5]   \n",
       "6  interesting work additional tool investigation...        en  [uidx_6]   \n",
       "7                       thank much fun talk research        en  [uidx_7]   \n",
       "8  ha day job turbulence model computational flui...        en  [uidx_8]   \n",
       "9  dr hutchison speak cannabis health course cour...        en  [uidx_9]   \n",
       "\n",
       "        similar_idx   dup_similar_idx dup_cluster_id  \n",
       "0          [uidx_0]         (uidx_0,)          dup_1  \n",
       "1  [uidx_1, uidx_3]  (uidx_1, uidx_3)          dup_2  \n",
       "2          [uidx_2]         (uidx_2,)          dup_3  \n",
       "3  [uidx_1, uidx_3]  (uidx_1, uidx_3)          dup_2  \n",
       "4          [uidx_4]         (uidx_4,)          dup_4  \n",
       "5          [uidx_5]         (uidx_5,)          dup_5  \n",
       "6          [uidx_6]         (uidx_6,)          dup_6  \n",
       "7          [uidx_7]         (uidx_7,)          dup_7  \n",
       "8          [uidx_8]         (uidx_8,)          dup_8  \n",
       "9          [uidx_9]         (uidx_9,)          dup_9  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>Processed_tweet</th>\n",
       "      <th>lang_mask</th>\n",
       "      <th>dup_idx</th>\n",
       "      <th>similar_idx</th>\n",
       "      <th>dup_similar_idx</th>\n",
       "      <th>dup_cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uidx_0</td>\n",
       "      <td>All campus dining locations are closed today, ...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>campus dining location close today jan 1 happy...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_0]</td>\n",
       "      <td>[uidx_0]</td>\n",
       "      <td>(uidx_0,)</td>\n",
       "      <td>dup_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uidx_1</td>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>what your campus dining option today jan 2 alf...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_1]</td>\n",
       "      <td>[uidx_1, uidx_3]</td>\n",
       "      <td>(uidx_1, uidx_3)</td>\n",
       "      <td>dup_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uidx_2</td>\n",
       "      <td>#FPGA Design for #Embedded #Systems\\n\\n#SoC #V...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>fpga design embed system soc verilog vlsi asic...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_2]</td>\n",
       "      <td>[uidx_2]</td>\n",
       "      <td>(uidx_2,)</td>\n",
       "      <td>dup_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uidx_4</td>\n",
       "      <td>As an anthro PhD student I‚Äôm frequently asked ...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>anthro phd student m frequently ask why study ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_4]</td>\n",
       "      <td>[uidx_4]</td>\n",
       "      <td>(uidx_4,)</td>\n",
       "      <td>dup_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uidx_5</td>\n",
       "      <td>True or False: Concussions for college student...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>true false concussion college student signific...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_5]</td>\n",
       "      <td>[uidx_5]</td>\n",
       "      <td>(uidx_5,)</td>\n",
       "      <td>dup_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uidx_6</td>\n",
       "      <td>@michaelgrandner @CUBoulder Very interesting w...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>interesting work additional tool investigation...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_6]</td>\n",
       "      <td>[uidx_6]</td>\n",
       "      <td>(uidx_6,)</td>\n",
       "      <td>dup_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uidx_7</td>\n",
       "      <td>@ShellyMBoulder Thanks so much @ShellyMBoulder...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>thank much fun talk research</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_7]</td>\n",
       "      <td>[uidx_7]</td>\n",
       "      <td>(uidx_7,)</td>\n",
       "      <td>dup_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uidx_8</td>\n",
       "      <td>@LiberalsAreMean @CUBoulder Ha! In my day job,...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>ha day job turbulence model computational flui...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_8]</td>\n",
       "      <td>[uidx_8]</td>\n",
       "      <td>(uidx_8,)</td>\n",
       "      <td>dup_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uidx_9</td>\n",
       "      <td>Dr. Hutchison speaks to @5280Magazine about th...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>dr hutchison speak cannabis health course cour...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_9]</td>\n",
       "      <td>[uidx_9]</td>\n",
       "      <td>(uidx_9,)</td>\n",
       "      <td>dup_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uidx_10</td>\n",
       "      <td>Could fluid dynamics research pave the way for...</td>\n",
       "      <td>cu_research</td>\n",
       "      <td>could fluid dynamic research pave way intraven...</td>\n",
       "      <td>en</td>\n",
       "      <td>[uidx_10]</td>\n",
       "      <td>[uidx_10]</td>\n",
       "      <td>(uidx_10,)</td>\n",
       "      <td>dup_10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UID                                              tweet        label  \\\n",
       "0   uidx_0  All campus dining locations are closed today, ...    cu_others   \n",
       "1   uidx_1  What are your campus dining options today, Jan...    cu_others   \n",
       "2   uidx_2  #FPGA Design for #Embedded #Systems\\n\\n#SoC #V...    cu_online   \n",
       "3   uidx_4  As an anthro PhD student I‚Äôm frequently asked ...  cu_research   \n",
       "4   uidx_5  True or False: Concussions for college student...  cu_research   \n",
       "5   uidx_6  @michaelgrandner @CUBoulder Very interesting w...  cu_research   \n",
       "6   uidx_7  @ShellyMBoulder Thanks so much @ShellyMBoulder...  cu_research   \n",
       "7   uidx_8  @LiberalsAreMean @CUBoulder Ha! In my day job,...  cu_research   \n",
       "8   uidx_9  Dr. Hutchison speaks to @5280Magazine about th...    cu_online   \n",
       "9  uidx_10  Could fluid dynamics research pave the way for...  cu_research   \n",
       "\n",
       "                                     Processed_tweet lang_mask    dup_idx  \\\n",
       "0  campus dining location close today jan 1 happy...        en   [uidx_0]   \n",
       "1  what your campus dining option today jan 2 alf...        en   [uidx_1]   \n",
       "2  fpga design embed system soc verilog vlsi asic...        en   [uidx_2]   \n",
       "3  anthro phd student m frequently ask why study ...        en   [uidx_4]   \n",
       "4  true false concussion college student signific...        en   [uidx_5]   \n",
       "5  interesting work additional tool investigation...        en   [uidx_6]   \n",
       "6                       thank much fun talk research        en   [uidx_7]   \n",
       "7  ha day job turbulence model computational flui...        en   [uidx_8]   \n",
       "8  dr hutchison speak cannabis health course cour...        en   [uidx_9]   \n",
       "9  could fluid dynamic research pave way intraven...        en  [uidx_10]   \n",
       "\n",
       "        similar_idx   dup_similar_idx dup_cluster_id  \n",
       "0          [uidx_0]         (uidx_0,)          dup_1  \n",
       "1  [uidx_1, uidx_3]  (uidx_1, uidx_3)          dup_2  \n",
       "2          [uidx_2]         (uidx_2,)          dup_3  \n",
       "3          [uidx_4]         (uidx_4,)          dup_4  \n",
       "4          [uidx_5]         (uidx_5,)          dup_5  \n",
       "5          [uidx_6]         (uidx_6,)          dup_6  \n",
       "6          [uidx_7]         (uidx_7,)          dup_7  \n",
       "7          [uidx_8]         (uidx_8,)          dup_8  \n",
       "8          [uidx_9]         (uidx_9,)          dup_9  \n",
       "9         [uidx_10]        (uidx_10,)         dup_10  "
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. `tweets.csv` - original data file\n",
    "\n",
    "\n",
    "- 2. `processed_Tweets.csv` - pre-processed file including dup and near-dup ids.\n",
    "\n",
    "\n",
    "- 3. `cluster_finaldata_Tweets.csv` - final pre-processed file for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Our preprocessed dataset has #records:  500\n",
      "-> Our dataset to be clustered has #records:  456\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed Dataset\n",
    "df = pd.read_csv(os.path.join(data_dir, \"tweets_preprocessed.csv\"))\n",
    "print(\"-> Our preprocessed dataset has #records: \", df.shape[0])\n",
    "\n",
    "# Final Dataset to be clustered\n",
    "df_cluster = pd.read_csv(os.path.join(data_dir, \"tweets_cluster_final.csv\"))\n",
    "print(\"-> Our dataset to be clustered has #records: \", df_cluster.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clustering\n",
    "\n",
    "Clustering Algorithm:\n",
    "\n",
    "1. Load configuration file - identify the column names (e.g. source_) that must be present in the input data schema.\n",
    "2. Update clustering configuration.\n",
    "3. Load the input file (csv, excel).\n",
    "4. Run Similarity computation using Fast Bert to identify duplicate and very similar records in the input data.\n",
    "5. Perform clustering only on unique records (dups dropped)\n",
    "    \n",
    "    5.1. Execute `run_clustering` - which accepts a 'preprocess' param to accept processed or un-processed text. \n",
    "    \n",
    "    5.2. Prepare input data: if _preporcess=True_, execute a preprocessing module `cluster_preprocessing_run_pipeline`.\n",
    "    \n",
    "    5.3. Run `doc_clustering` to perform each iteration of clustering algorithm\n",
    "    \n",
    "    5.3. Use other helper modules `cluster_cohesion_cosine` and `cluster_center_sent` to facilate clustering process.\n",
    "    \n",
    "    5.4. Return cluster information on unique records.\n",
    "    \n",
    "6. Merge the original dataframe (with dups and very similar records) with clustered data on unique rows.\n",
    "7. Consolidate cluster information on all records with above merged data.\n",
    "8. Create visual reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithm and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "code_folding": [
     3,
     78,
     107,
     164,
     185,
     205,
     245,
     257
    ]
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#  Clustering Utilities...\n",
    "####################################################################################\n",
    "\n",
    "# preprocessing text - Only runs when: run_clustering(df_cluster, spacy_model, preprocess=True, vectorizer='bert')\n",
    "def cluster_preprocessing_run_pipeline(input_json, spacy_model):\n",
    "    \"\"\"\n",
    "    Runs Spacy pipeline specified by params.\n",
    "    param: input_json = {'text': sentence, 'props': props_str}\n",
    "    return: output_json\n",
    "    \"\"\"\n",
    "    # Get all parameters from input JSON\n",
    "    text = input_json[\"text\"]\n",
    "    for prop in input_json[\"props\"]:\n",
    "        if prop[\"key\"] == \"annotators\":\n",
    "            operations = prop[\"value\"].split(\",\")\n",
    "\n",
    "    # Run Spacy Pipeline\n",
    "    doc = spacy_model(text)\n",
    "    doc_json = doc.to_json()  # Includes all info from spacy pipeline\n",
    "\n",
    "    output_json = {}\n",
    "    output_json[\"sentences\"] = []\n",
    "\n",
    "    # a. create necessary dictionaries\n",
    "    # a.1 Extract Entity List\n",
    "    entity_list = doc_json[\"ents\"]\n",
    "    # a.2 create token lib\n",
    "    token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "    # b. add sentence indices\n",
    "    for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "        out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "        parse = \"\"\n",
    "        basicDependencies = []\n",
    "        output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "        # c. split sentences by indices, add labels (pos, ner, dep, etc.)\n",
    "        for token in doc_json[\"tokens\"]:\n",
    "            if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                # Extract Entity label\n",
    "                ner = \"O\"\n",
    "                for entity in entity_list:\n",
    "                    if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                        ner = entity[\"label\"]\n",
    "                # Extract dependency info\n",
    "                dep = token[\"dep\"]\n",
    "                governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]: token_lib[token[\"head\"]][\"end\"]]\n",
    "                dependent = token[\"id\"] + 1\n",
    "                dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "                lemma = doc[token[\"id\"]].lemma_\n",
    "                # d. add dependencies\n",
    "                basicDependencies.append({\"dep\": dep,\n",
    "                                          \"governor\": governor,\n",
    "                                          \"governorGloss\": governorGloss,\n",
    "                                          \"dependent\": dependent,\n",
    "                                          \"dependentGloss\": dependentGloss})\n",
    "                # e. add tokens\n",
    "                out_token = {\"index\": token[\"id\"] + 1,\n",
    "                             \"word\": dependentGloss,\n",
    "                             \"originalText\": dependentGloss,\n",
    "                             \"characterOffsetBegin\": token[\"start\"],\n",
    "                             \"characterOffsetEnd\": token[\"end\"]}\n",
    "                if \"lemma\" in operations:\n",
    "                    out_token[\"lemma\"] = lemma\n",
    "                if \"pos\" in operations:\n",
    "                    out_token[\"pos\"] = token[\"tag\"]\n",
    "                if \"ner\" in operations:\n",
    "                    out_token[\"ner\"] = ner\n",
    "                out_sentence[\"tokens\"].append(out_token)\n",
    "        if \"parse\" in operations:\n",
    "            out_sentence[\"parse\"] = parse\n",
    "            out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "            out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "            out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "\n",
    "    return output_json\n",
    "\n",
    "# cohesion calcuation\n",
    "def cluster_cohesion_cosine(clustering, cluster_set_by_sid, data):\n",
    "    '''\n",
    "    :: COESHION SCORE ::\n",
    "    Calculates cohesion score for each cluster id. Accepts 1 cluster with all members, and re-runs clustering on them,\n",
    "    with k=1, e.g. KMeans(cluster_id_1_documents, k=1). Calculates relative distances between each member.\n",
    "    '''\n",
    "    document_center = []\n",
    "    values = []\n",
    "    for sid in cluster_set_by_sid:\n",
    "        document_center.append(data[sid]['sent_lemma'])\n",
    "\n",
    "    # clustering one cluster members with k=1 to find relatvie distances\n",
    "    model_center, cluster_labels_, cluster_centers_center, X_center = clustering.run_doc_clustering(document_center, 1)\n",
    "\n",
    "    for i in range(0, len(cluster_set_by_sid)):\n",
    "        d_vec = X_center[i].toarray()\n",
    "        simval = spatial.distance.cosine(d_vec[0], cluster_centers_center[0,:])\n",
    "        if math.isnan(simval):\n",
    "            values.append(0)\n",
    "        else:\n",
    "            values.append(1 - simval)\n",
    "    if len(values) == 0:\n",
    "        cohesion_score = 0\n",
    "    else:\n",
    "        # avg distance of all members\n",
    "        cohesion_score = sum(values) / len(values)\n",
    "    return cohesion_score\n",
    "\n",
    "# centorid calcuation\n",
    "def cluster_center_sent(clustering, cluster_set_by_sid, data):\n",
    "    '''\n",
    "    :: CLUSTER CENTRE ::\n",
    "    Calculates cluster centre using model capability to return cluster centre values, and calculates closest two members.\n",
    "    '''\n",
    "    document_center = []\n",
    "    sid_mapping_center = {}\n",
    "    sid_tmp = 0\n",
    "    sent_num_cid = len(cluster_set_by_sid)\n",
    "\n",
    "    dup_sid_set = []\n",
    "    for sid in cluster_set_by_sid:\n",
    "        sid_mapping_center[sid_tmp] = sid\n",
    "        if len(data[sid]['sent_lemma']) > 0:\n",
    "            dup_sid_set.append(sid)\n",
    "        document_center.append(data[sid]['sent_lemma'])\n",
    "        sid_tmp +=1\n",
    "\n",
    "    if len(dup_sid_set) == 1:\n",
    "        seedid1 = list(dup_sid_set)[0]\n",
    "        return seedid1, seedid1\n",
    "\n",
    "    # clustering one cluster members with k=1 to find relatvie distances\n",
    "    model_center, cluster_labels_, cluster_centers_center, X_center = clustering.run_doc_clustering(document_center, 1)\n",
    "\n",
    "    DISTMIN = -9999\n",
    "\n",
    "    values = []\n",
    "    values_sid_set = []\n",
    "    # remove duplicated ones\n",
    "    for i in range(0, sent_num_cid):\n",
    "        sid_data = sid_mapping_center[i]\n",
    "        if data[sid_data][\"count\"] > 0:\n",
    "            d_vec = X_center[i].toarray()\n",
    "            # values.append(spatial.distance.euclidean(d_vec, cluster_centers_center[0,:]))\n",
    "            if math.isnan(spatial.distance.cosine(d_vec[0], cluster_centers_center[0,:])):\n",
    "                values.append(1)\n",
    "            else:\n",
    "                values.append(1 - spatial.distance.cosine(d_vec[0], cluster_centers_center[0,:]))\n",
    "            # score = 1 - spatial.distance.cosine(d_vec, cluster_centers[int(cid),:]) #mainid\n",
    "            values_sid_set.append(sid_data)\n",
    "\n",
    "    seedid1_tmp = values.index(max(values))\n",
    "    min_dist1 = values[seedid1_tmp]\n",
    "    values[seedid1_tmp] = DISTMIN\n",
    "    seedid1 = values_sid_set[ seedid1_tmp ]\n",
    "\n",
    "    seedid2_tmp = values.index(max(values))\n",
    "    min_dist2 = values[seedid2_tmp]\n",
    "    seedid2 = values_sid_set[ seedid2_tmp ]\n",
    "    values[seedid1_tmp] = min_dist1\n",
    "\n",
    "    return seedid1, seedid2, values, values_sid_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "#  Clustering Algorithm\n",
    "####################################################################################\n",
    "\n",
    "class doc_clustering():\n",
    "\n",
    "    def __init__(self, vectorizer, config):\n",
    "        \n",
    "        # :: clustering config ::\n",
    "        self.clustering_type = \"noun\"\n",
    "        self.cluster_k = config.get('kmeans_k')\n",
    "        self.kmeans_rate = config.get('kmeans_rate')\n",
    "        self.kmeans_seed_init = config.get('kmeans_seedInit')\n",
    "        self.kmeans_maxiter = config.get('kmeans_maxIter')\n",
    "        self.cluster_length = config.get('kmeans_clusterLen')\n",
    "        self.cohesion_threshold = config.get('kmeans_cohesion_threshold')\n",
    "        self.vectorizer = vectorizer.lower().strip()\n",
    "        if self.vectorizer not in ['tfidf', 'bert', 'word2vec', 'doc2vec']:\n",
    "            raise Exception(\"Invalid vectorizer passed!\")\n",
    "        self.vectorizer_model_fp = config['vectorizer_model_fp'][self.vectorizer]\n",
    "        \n",
    "        # :: resource config ::\n",
    "        self.stopwd_file = os.path.join(config.get('nlp_resources_fp'), \"stopwords.txt\")\n",
    "        self.spacy_model = nlp\n",
    "        self.stopwords = []\n",
    "        self.test_mode = \"no\"\n",
    "        \n",
    "        # :: CUSTOM DOMAIN SYNONYMS ::\n",
    "        self.synonym_file = os.path.join(config.get('nlp_resources_fp'), \"synonyms_noun_verb.txt\")\n",
    "        self.use_custom_synonyms = True if os.path.exists(self.synonym_file) else False\n",
    "        self.synonym_dict = {}\n",
    "        print(\"Custom Synonyms Mapping (on/off): \", self.use_custom_synonyms)\n",
    "        \n",
    "        self.load_resources()\n",
    "        return\n",
    "\n",
    "    def load_resources(self):\n",
    "        # load stopwords\n",
    "        with io.open(self.stopwd_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        \n",
    "        ####################################################################\n",
    "        # load CUSTOM DOMAIN SYNONYMS\n",
    "        ####################################################################\n",
    "        if self.use_custom_synonyms:\n",
    "            synonym_list = []\n",
    "            with io.open(self.synonym_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                synonym_list = [x.rstrip() for x in f.readlines()]\n",
    "                for line in synonym_list:\n",
    "                    if len(line) < 4 or re.search(\"^#\", line):\n",
    "                        continue\n",
    "                    syn_words = line.lower().split(\"\\t\")\n",
    "                    pos = syn_words[0]\n",
    "                    hwd = syn_words[1]\n",
    "                    syn_words.pop(0)\n",
    "                    syn_words.pop(0)\n",
    "                    for s in syn_words:\n",
    "                        self.synonym_dict.setdefault(s, {})[pos] = hwd\n",
    "        else:\n",
    "            pass\n",
    "        ####################################################################\n",
    "        return\n",
    "\n",
    "    def get_pos_list(self, results):\n",
    "        # POS Annotation using Spacy\n",
    "        pos_ans = []\n",
    "        word_list = []\n",
    "        lemma_list = []\n",
    "        ne_list = []\n",
    "        start_end_list = []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                pos_syn = \"\"\n",
    "                pos = token['pos'].lower()\n",
    "                pos_ans.append(token['pos'])\n",
    "                word_list.append(token['word'])\n",
    "                lemma = token['lemma'].lower()\n",
    "                if lemma in self.stopwords and lemma not in ['want', 'against', 'further', 'online', 'same', 'under',\n",
    "                                                             'what', 'want', 'when', 'own', ''] \\\n",
    "                        or lemma in [\":\", \"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"\\\\\", '-pron-', '_', 'card num', '\"'] \\\n",
    "                        or pos == \":\" or pos == \".\" or re.search('^([\\W]*)$]', lemma) or len(lemma) >= 30:\n",
    "                    continue\n",
    "                if re.search('^nn', pos):\n",
    "                    pos_syn = 'noun'\n",
    "                elif re.search('^v', pos):\n",
    "                    pos_syn = 'verb'\n",
    "                elif re.search('^adj', pos):\n",
    "                    pos_syn = 'adj'\n",
    "                \n",
    "                ####################################################################\n",
    "                # use CUSTOM DOMAIN SYNONYMS\n",
    "                # replace lemma with primary synonym key!\n",
    "                ####################################################################\n",
    "                if self.use_custom_synonyms:\n",
    "                    if lemma in list(self.synonym_dict.keys()) and pos_syn in list(self.synonym_dict[lemma].keys()):\n",
    "                        lemma = self.synonym_dict[lemma][pos_syn]\n",
    "                ####################################################################\n",
    "                \n",
    "                lemma_list.append(lemma)\n",
    "                ne_list.append(token['ner'])\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "\n",
    "        if len(lemma_list) == 0:\n",
    "            sent_lemma = \"NO_NP_KEYS\"\n",
    "        else:\n",
    "            sent_lemma = \" \".join(lemma_list)\n",
    "        return \" \".join(word_list), sent_lemma, \" \", \" \"\n",
    "\n",
    "    def get_sent_lemma(self, sent):\n",
    "        # allowed spacy attributes\n",
    "        props_str = [{'value': 'false', 'key': 'enforceRequirements'},\n",
    "                     {'value': 'json', 'key': 'outputFormat'},\n",
    "                     {'value': 'normalizeSpace=false, strictTreebank3=true', 'key': 'tokenize.options'},\n",
    "                     {'value': 'tokenize,ssplit,pos,lemma,ner', 'key': 'annotators'},\n",
    "                     {'value': 'true', 'key': 'ssplit.eolonly'}]\n",
    "        input_json = {'text': sent, 'props': props_str}\n",
    "        results = cluster_preprocessing_run_pipeline(input_json, self.spacy_model)\n",
    "        return self.get_pos_list(results)\n",
    "\n",
    "\n",
    "    def run_doc_clustering(self, documents, k_val, run_analysis=False):\n",
    "\n",
    "        max_iter = self.kmeans_maxiter\n",
    "        ninit = self.kmeans_seed_init\n",
    "        r_num = len(documents) - 1\n",
    "\n",
    "        ################################################################################\n",
    "        # Vectorization Engine\n",
    "        ################################################################################\n",
    "        if self.vectorizer==\"tfidf\":\n",
    "            print(\"Vectorizing using...: tfidf records: \", len(documents))\n",
    "            vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 1))\n",
    "            X = vectorizer.fit_transform(documents)\n",
    "        \n",
    "        elif self.vectorizer==\"word2vec\":\n",
    "            print(\"Vectorizing using...: Word2Vec records: \", len(documents))\n",
    "            raise Exception(\"Word2vec is not yet defined! Define first in 'Word2vec_Vecotrizer' !\")\n",
    "            \n",
    "        elif self.vectorizer==\"doc2vec\":\n",
    "            print(\"Vectorizing using...: Doc2Vec records: \", len(documents))\n",
    "            raise Exception(\"Doc2vec is not yet defined! Define first in 'Doc2vec_Vecotrizer' !\")\n",
    "                \n",
    "        elif self.vectorizer==\"bert\":\n",
    "            if not self.vectorizer_model_fp:\n",
    "                raise Exception(\"For using sBERT, you must define model-path in the config!\")\n",
    "            print(\"Vectorizing using...: sBert records: \", len(documents))\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.vectorizer_model_fp)\n",
    "            model_bert = AutoModel.from_pretrained(self.vectorizer_model_fp)\n",
    "            vectorizer = Bert_Vectorizer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n",
    "            X = vectorizer.transform(documents, formatt='csr')\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Please specify a vectorizer from available: ['tfidf', 'word2vec', 'doc2vec', 'bert']\")\n",
    "        print(\"Vectorization complete!\")\n",
    "        ################################################################################\n",
    "\n",
    "        # Value of \"K\" clusters\n",
    "        true_k = k_val = int(k_val)\n",
    "\n",
    "        ## --> Heusristics <--\n",
    "        if self.clustering_type == \"fixed\": true_k = 2000\n",
    "        elif self.clustering_type == \"noun\": true_k = int(np.sqrt(len(documents) / 2)) * self.kmeans_rate\n",
    "        elif self.clustering_type == \"verb\": true_k = int(np.sqrt(len(documents) / 2))\n",
    "        else: print(\"Please set up clustering type! Options=['fixed', 'noun', 'verb']\")\n",
    "        print(\"Heusristic value of K: true_k=\", true_k)\n",
    "\n",
    "        ## --> Elbow method (silhouette) <--\n",
    "        if run_analysis and k_val==0 and true_k>=25:\n",
    "            search_range = range(true_k-20, true_k+20, 1)\n",
    "            sil_score_max = -1 # minimum possible score\n",
    "            for n_clusters in search_range:\n",
    "                model = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=2000, random_state=r_num, n_init=ninit)\n",
    "                labels = model.fit_predict(X)\n",
    "                sil_score = silhouette_score(X, labels)\n",
    "                print(\"The average silhouette score for %i clusters is %0.2f\" %(n_clusters, sil_score))\n",
    "                if sil_score > sil_score_max:\n",
    "                    true_k, sil_score_max = n_clusters, sil_score\n",
    "            print(\"Silhouette Analysis completed! Best_n_clusters K=\", true_k)\n",
    "\n",
    "        if k_val > 0 and self.clustering_type == \"noun\":\n",
    "            if len(documents) / 2 < k_val:\n",
    "                debug_msg = \"Number of input lines: {}\\n\" + \\\n",
    "                             \"K value provided: {}\\n\" + \\\n",
    "                             \"Suggested value of K for the 1st level clustering is less than input-lines divided by 2\\n\" + \\\n",
    "                             \"Please try again!  Heusristic value of K (if line number > 100): {}\"\\\n",
    "                            .format(len(documents), k_val, int(np.sqrt(len(documents)/2))*self.kmeans_rate)\n",
    "                raise Exception(debug_msg)\n",
    "            else:\n",
    "                true_k = k_val\n",
    "\n",
    "\n",
    "        # Final Clustering\n",
    "        if true_k == 0: true_k = 1\n",
    "        model = KMeans(n_clusters=true_k, init='k-means++', max_iter=2000, random_state=r_num, n_init=ninit)\n",
    "        model.fit(X)\n",
    "\n",
    "        cluster_labels = model.labels_\n",
    "        cluster_centers = model.cluster_centers_\n",
    "        print(\"Clustering is complete.\")\n",
    "\n",
    "        return model, cluster_labels, cluster_centers, X\n",
    "\n",
    "\n",
    "\n",
    "## SAMPLE EXECUTION\n",
    "\n",
    "# clustering_obj = doc_clustering(nlp_resources_fp, spacy_model, vectorize=['tfidf', 'bert'])\n",
    "# sent_tokens, sent_lemma, sent_verb, sent_dup = clustering_obj.get_sent_lemma(input_sentence)\n",
    "# model, cluster_labels, cluster_centers, X_vectorized = clustering.run_doc_clustering(sentences, cluster_k, run_analysis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "code_folding": [
     0,
     17,
     50,
     56,
     58,
     83,
     98,
     148,
     165
    ]
   },
   "outputs": [],
   "source": [
    "def run_clustering(df_cluster, config, preprocess=False, vectorizer='bert'):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Config parameters to be used\n",
    "    col_id = config.get('colname_id')\n",
    "    col_clean_txt = config.get('colname_clean_txt')\n",
    "    col_dup_sim_id = config.get('colname_dup_similar_id_col')\n",
    "    col_cluster_id = config.get('colname_cluster_id')\n",
    "    \n",
    "    ################################################################################################\n",
    "    # 1. INIT DOC CLUSTERING CLASS OBJECT\n",
    "    ################################################################################################\n",
    "    clustering = doc_clustering(vectorizer, config)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # 2. PREPARE DATA (Optional: preprocessing)\n",
    "    ################################################################################################\n",
    "    print(\"Uploading input file and lemmatizing...\")\n",
    "    # create 'data' dict to store every info regarding each sentence\n",
    "    data = OrderedDict()\n",
    "    for index, row in df_cluster.iterrows():\n",
    "        uid, input_text, dup_similar_idx = row[col_id], row[col_clean_txt], row[col_dup_sim_id]\n",
    "        if len(input_text.strip())==0: \n",
    "            input_text = 'NONE'\n",
    "        original_text = input_text.strip()\n",
    "        \n",
    "        if preprocess:\n",
    "            # run spacy complete pipeline\n",
    "            sent_tokens, sent_lemma, sent_verb, sent_dup = clustering.get_sent_lemma(original_text)\n",
    "            input_text = str(sent_lemma)\n",
    "\n",
    "        # features\n",
    "        upper_cnt = sum(1 for c in input_text if c.isupper())\n",
    "        case_ratio = float(upper_cnt) / float(len(input_text))\n",
    "        data.setdefault(index, {})[\"sent_lemma\"] = str(input_text)\n",
    "        # :not: data.setdefault(index, {})[\"sent_noun\"] = str(sent_lemma)\n",
    "        # :not: data.setdefault(index, {})[\"sent_verb\"] = str(sent_verb)\n",
    "        # :not: data.setdefault(index, {})[\"sent_tokens\"] = sent_tokens\n",
    "        data.setdefault(index, {})[\"count\"] = len(dup_similar_idx)\n",
    "        data.setdefault(index, {})[\"case_ratio\"] = case_ratio\n",
    "        data.setdefault(index, {})[\"clusterid\"] = \"no\"\n",
    "        data.setdefault(index, {})[\"keep\"] = \"yes\"\n",
    "        data.setdefault(index, {})[\"select\"] = \"no\"\n",
    "        data.setdefault(index, {})[\"index\"] = index\n",
    "        data.setdefault(index, {})[\"unique_id\"] = uid\n",
    "        data.setdefault(index, {})[\"sent_raw\"] = original_text\n",
    "\n",
    "    ################################################################################################\n",
    "    # 3. PREPARE CORPUS\n",
    "    ################################################################################################\n",
    "    print(\"Collecting text corpus\")\n",
    "    documents = []\n",
    "    main_docid_id_map = {}  # id in documents vs real id in input\n",
    "    doc_id = 0\n",
    "    for id, x in list(data.items()):\n",
    "        if data[id][\"count\"] >= 0:\n",
    "            main_docid_id_map[doc_id] = id\n",
    "            documents.append(data[id]['sent_lemma'])\n",
    "            doc_id += 1\n",
    "    if len(documents) < 50:\n",
    "        raise Exception(\"The file contains sentences less than 50! Please check  and run again!\")\n",
    "    else:\n",
    "        print('>> Total input sentences = ', len(documents))\n",
    "\n",
    "    ################################################################################################\n",
    "    # 4. 1st LEVEL CLUSTERING\n",
    "    ################################################################################################\n",
    "    print(\"\\nPerforming 1st level clustering\")\n",
    "\n",
    "    # init, using heursitics to find value of k\n",
    "    cluster_k = 0\n",
    "\n",
    "    clustering.clustering_type = \"noun\"\n",
    "    model, cluster_labels, cluster_centers, X = clustering.run_doc_clustering(documents, cluster_k, run_analysis=False)\n",
    "    \n",
    "    # cluster_id : question id\n",
    "    c_s_list = [(y,x) for x,y in enumerate(cluster_labels)]\n",
    "\n",
    "    ################################################################################################\n",
    "    # 5. COLLECT CLUSTERED IDs\n",
    "    ################################################################################################\n",
    "    print(\"\\nCollecting clustered ids\")\n",
    "\n",
    "    # add first level cluster-id to data dict --> cluster_id : [Q_id1, Q_id2, ..., Q_idn]\n",
    "    cid_sid_counter = {}\n",
    "    for cid, sid in c_s_list:\n",
    "        main_sid = main_docid_id_map[sid]\n",
    "        data.setdefault(main_sid, {})[\"clusterid\"] = str(format(cid, \"05d\"))\n",
    "        cid_sid_counter.setdefault(cid, []).append(main_sid)\n",
    "\n",
    "    ################################################################################################\n",
    "    # 6. CHECK REQUIREMENT FOR 2nd LEVEL CLUSTERING\n",
    "    ################################################################################################\n",
    "    print(\"\\nFine-tuning clusters: Checking if 2nd level clustering is required...\")\n",
    "\n",
    "    clustering.clustering_type = \"noun\"\n",
    "    cohesionThreshold = clustering.cohesion_threshold\n",
    "    clusteringMaxLen = clustering.cluster_length\n",
    "    print('cohesionThreshold is set to: ', cohesionThreshold)\n",
    "\n",
    "    for cid in sorted(cid_sid_counter.keys()):\n",
    "\n",
    "        # --> documents under cluster_id = 'cid'\n",
    "        documents_sub =[]\n",
    "        for sid in cid_sid_counter[cid]:\n",
    "            documents_sub.append(data[sid]['sent_lemma'])\n",
    "\n",
    "        if len(documents_sub) > clusteringMaxLen:\n",
    "            print(1, documents_sub)\n",
    "            cohesion_score = cluster_cohesion_cosine(clustering, cid_sid_counter[cid], data)\n",
    "            print('cid -- cohesion_score = ', cid, cohesion_score)\n",
    "\n",
    "            # RE-CLUSTERING   (to use cohesion value for basis of reclustering)\n",
    "            if cohesion_score < cohesionThreshold:\n",
    "                print('**re-clustering: cluster_id={}; member count={}; cohesion={}'.format(cid, len(documents_sub), cohesion_score))\n",
    "                model_sub, cluster_labels_sub, cluster_centre_sub, X_sub = clustering.run_doc_clustering(documents_sub, cluster_k)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # :: ONLY if 2nd clustering was performed ::\n",
    "        c_s_list_sub = [(y,x) for x,y in enumerate(cluster_labels_sub)]\n",
    "\n",
    "        # add 2nd level cluster-id to data dict\n",
    "        # --> cluster_id : [Q_id1, Q_id2, ..., Q_idn]\n",
    "        for cid_sub,sid_sub in c_s_list_sub:\n",
    "            sid_index = cid_sid_counter[cid][sid_sub]\n",
    "            data[sid_index]['clusterid'] = data[sid_index]['clusterid'] + \"_\"+str(format(cid_sub, \"03d\"))\n",
    "\n",
    "    # reset to first level\n",
    "    clustering.clustering_type = \"noun\"\n",
    "    cluster_set_by_sid = {}\n",
    "    for id, x in list(data.items()):\n",
    "        if data[id]['clusterid'] != \"no\":\n",
    "            cid = data[id]['clusterid']\n",
    "            cluster_set_by_sid.setdefault(cid, []).append(id)\n",
    "\n",
    "    ################################################################################################\n",
    "    # 7. FILTER, FIND INSIGHTS & SAVE\n",
    "    ################################################################################################\n",
    "    print(\"\\nFiltering, getting insights and saving output\")\n",
    "\n",
    "    # SAVE OUTPUT (CSV)\n",
    "    output = []\n",
    "    output.append(\"{}\\t{}\\tText\\tSeed_Q1\\tSeed_Q2\\tDist_to_Center\\tCohesion\\r\\n\".format(col_cluster_id, col_id))\n",
    "\n",
    "    T = 0.0  # recommended: 0.4\n",
    "    cid_index = 0\n",
    "    cluster_set_by_sid_filtered = {}\n",
    "    cluster_set_by_sid_filtered_coh_score = {}\n",
    "    for cid in list(cluster_set_by_sid.keys()):\n",
    "        if len(cluster_set_by_sid[cid]) > 1:\n",
    "            cohesion_score = cluster_cohesion_cosine(clustering, cluster_set_by_sid[cid], data)\n",
    "            if cohesion_score < T:\n",
    "                for s in cluster_set_by_sid[cid]:\n",
    "                    cluster_set_by_sid_filtered.setdefault(cid_index,[]).append(s)\n",
    "                    cid_index += 1\n",
    "            else:\n",
    "                cluster_set_by_sid_filtered[cid_index] = cluster_set_by_sid[cid]\n",
    "                cluster_set_by_sid_filtered_coh_score[cid_index] = cohesion_score\n",
    "                cid_index += 1\n",
    "        else:\n",
    "            cluster_set_by_sid_filtered[cid_index] = cluster_set_by_sid[cid]\n",
    "            cluster_set_by_sid_filtered_coh_score[cid_index] = 0\n",
    "            cid_index += 1\n",
    "\n",
    "    output_index=0\n",
    "    for cid in list(cluster_set_by_sid_filtered.keys()):\n",
    "        sent_num_cid = len(cluster_set_by_sid_filtered[cid])\n",
    "        values = []\n",
    "        values_sid = []\n",
    "\n",
    "        if sent_num_cid > 0 and len(cluster_set_by_sid_filtered[cid]) > 1:\n",
    "            seedid1,seedid2, values, values_sid = cluster_center_sent(clustering, cluster_set_by_sid_filtered[cid], data)\n",
    "        else:\n",
    "            seedid1 = cluster_set_by_sid_filtered[cid][0]\n",
    "            seedid2 = cluster_set_by_sid_filtered[cid][0]\n",
    "            values.append(1)\n",
    "            values_sid.append(seedid1)\n",
    "\n",
    "        seed_q1 = data[seedid1][\"sent_raw\"]\n",
    "        seed_q2 = data[seedid2][\"sent_raw\"]\n",
    "\n",
    "        sent_count = 0\n",
    "        for sid in cluster_set_by_sid_filtered[cid]:\n",
    "            sent_count += data[sid][\"count\"]\n",
    "        ch_score= cluster_set_by_sid_filtered_coh_score[cid]\n",
    "\n",
    "        for sid in cluster_set_by_sid_filtered[cid]:\n",
    "            dup_count = data[sid]['count']\n",
    "            if dup_count == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sid_norm = data[sid][\"index\"]\n",
    "                sent = data[sid_norm]['sent_raw']\n",
    "            sim_index = values_sid.index(sid)\n",
    "            dist_to_center = 1.0 - values[sim_index]\n",
    "            unique_id = data[sid][\"unique_id\"]\n",
    "\n",
    "            # cid, sid, unique_id, sent, seed_q1, seed_q2, dist_to_center, cohesion_score\n",
    "            output.append(\"%d\\t%s\\t%s\\t%s\\t%s\\t%5.3f\\t%5.3f\\r\\n\" % (cid, unique_id, sent, seed_q1, seed_q2, dist_to_center, ch_score))\n",
    "\n",
    "        cid_index += 1\n",
    "\n",
    "    clustered_output = pd.read_csv(StringIO(\"\\n\".join(output)), sep=\"\\t\").reset_index(drop=True)\n",
    "    print(\"\\nClustering process finished! Time taken(s) =\", (time.time()-start))\n",
    "\n",
    "    return clustered_output\n",
    "\n",
    "\n",
    "\n",
    "## SAMPLE EXECUTION\n",
    "\n",
    "# clustered_output = run_clustering(df_cluster, spacy_model, preprocess=False, vectorizer=['tfidf', 'bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Synonyms Mapping (on/off):  False\n",
      "Uploading input file and lemmatizing...\n",
      "Collecting text corpus\n",
      ">> Total input sentences =  456\n",
      "\n",
      "Performing 1st level clustering\n",
      "Vectorizing using...: tfidf records:  456\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 75\n",
      "Clustering is complete.\n",
      "\n",
      "Collecting clustered ids\n",
      "\n",
      "Fine-tuning clusters: Checking if 2nd level clustering is required...\n",
      "cohesionThreshold is set to:  0.7\n",
      "\n",
      "Filtering, getting insights and saving output\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  10\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  15\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  8\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  10\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  16\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  11\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  8\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  10\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  15\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  8\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  10\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  16\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  11\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  3\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  6\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  9\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  7\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  8\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  12\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 10\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  5\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  4\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "Vectorizing using...: tfidf records:  2\n",
      "Vectorization complete!\n",
      "Heusristic value of K: true_k= 5\n",
      "Clustering is complete.\n",
      "\n",
      "Clustering process finished! Time taken(s) = 14.30741810798645\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE CLUSTERING\n",
    "\n",
    "clustered_output = run_clustering(df_cluster, config, preprocess=False, vectorizer='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involves 2 layers of clustering, one initial with heruristic value of k and 2nd for fine-tuning any cluster having more than 'CLUSTER_LEN'members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>UID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Seed_Q1</th>\n",
       "      <th>Seed_Q2</th>\n",
       "      <th>Dist_to_Center</th>\n",
       "      <th>Cohesion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>uidx_0</td>\n",
       "      <td>campus dining location close today jan 1 happy...</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>uidx_235</td>\n",
       "      <td>move teach remote work begin monday begin</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>uidx_293</td>\n",
       "      <td>dining update order maintain health safety our...</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>uidx_311</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>uidx_323</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>73</td>\n",
       "      <td>uidx_309</td>\n",
       "      <td>stay date late dining option campus</td>\n",
       "      <td>stay date late dining option campus</td>\n",
       "      <td>sure stay date campus dining option via</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>73</td>\n",
       "      <td>uidx_312</td>\n",
       "      <td>ve enter next phase campus effectively close r...</td>\n",
       "      <td>stay date late dining option campus</td>\n",
       "      <td>sure stay date campus dining option via</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>73</td>\n",
       "      <td>uidx_365</td>\n",
       "      <td>colorado find 2nd well prepared state nation c...</td>\n",
       "      <td>stay date late dining option campus</td>\n",
       "      <td>sure stay date campus dining option via</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>74</td>\n",
       "      <td>uidx_466</td>\n",
       "      <td>stand colleague americas lead research univers...</td>\n",
       "      <td>stand colleague americas lead research univers...</td>\n",
       "      <td>see woman golf coach anne kelly stand colleagu...</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>74</td>\n",
       "      <td>uidx_467</td>\n",
       "      <td>see woman golf coach anne kelly stand colleagu...</td>\n",
       "      <td>stand colleague americas lead research univers...</td>\n",
       "      <td>see woman golf coach anne kelly stand colleagu...</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cluster_id       UID                                               Text  \\\n",
       "0             0    uidx_0  campus dining location close today jan 1 happy...   \n",
       "1             0  uidx_235          move teach remote work begin monday begin   \n",
       "2             0  uidx_293  dining update order maintain health safety our...   \n",
       "3             0  uidx_311  dining update begin today march 16 follow dini...   \n",
       "4             0  uidx_323  dining update seec cafe starbucks close begin ...   \n",
       "..          ...       ...                                                ...   \n",
       "451          73  uidx_309                stay date late dining option campus   \n",
       "452          73  uidx_312  ve enter next phase campus effectively close r...   \n",
       "453          73  uidx_365  colorado find 2nd well prepared state nation c...   \n",
       "454          74  uidx_466  stand colleague americas lead research univers...   \n",
       "455          74  uidx_467  see woman golf coach anne kelly stand colleagu...   \n",
       "\n",
       "                                               Seed_Q1  \\\n",
       "0    dining update seec cafe starbucks close begin ...   \n",
       "1    dining update seec cafe starbucks close begin ...   \n",
       "2    dining update seec cafe starbucks close begin ...   \n",
       "3    dining update seec cafe starbucks close begin ...   \n",
       "4    dining update seec cafe starbucks close begin ...   \n",
       "..                                                 ...   \n",
       "451                stay date late dining option campus   \n",
       "452                stay date late dining option campus   \n",
       "453                stay date late dining option campus   \n",
       "454  stand colleague americas lead research univers...   \n",
       "455  stand colleague americas lead research univers...   \n",
       "\n",
       "                                               Seed_Q2  Dist_to_Center  \\\n",
       "0    dining update begin today march 16 follow dini...           0.531   \n",
       "1    dining update begin today march 16 follow dini...           0.651   \n",
       "2    dining update begin today march 16 follow dini...           0.491   \n",
       "3    dining update begin today march 16 follow dini...           0.416   \n",
       "4    dining update begin today march 16 follow dini...           0.371   \n",
       "..                                                 ...             ...   \n",
       "451            sure stay date campus dining option via           0.226   \n",
       "452            sure stay date campus dining option via           0.490   \n",
       "453            sure stay date campus dining option via           0.538   \n",
       "454  see woman golf coach anne kelly stand colleagu...           0.065   \n",
       "455  see woman golf coach anne kelly stand colleagu...           0.065   \n",
       "\n",
       "     Cohesion  \n",
       "0       0.502  \n",
       "1       0.502  \n",
       "2       0.502  \n",
       "3       0.502  \n",
       "4       0.502  \n",
       "..        ...  \n",
       "451     0.616  \n",
       "452     0.616  \n",
       "453     0.616  \n",
       "454     0.935  \n",
       "455     0.935  \n",
       "\n",
       "[456 rows x 7 columns]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Final Matrix: Merge {original data + df_cluster + clustered_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_output(df, df_cluster, clustered_output):\n",
    "    col_id = config['colname_id']\n",
    "    col_dup_sim_id = config['colname_dup_similar_id_col']\n",
    "    col_cluster_freq = config['colname_freq']\n",
    "    col_cluster_coverage = config['colname_coverage']\n",
    "    col_dup_cluster_ids = config['colname_duplicate_cluster_id']\n",
    "    col_cluster_id = config['colname_cluster_id']\n",
    "    \n",
    "    # 1. MERGE 'clustered_output' WITH 'df_cluster' on UNIQUE_ID TO CREATE a 'temp'\n",
    "    #    --> maps 'dup_similar_idx' with 'cluster_id' i.e. {dup_similar_idx: Cluster_Info}\n",
    "    #\n",
    "    temp = df_cluster[[col_id, col_dup_sim_id]]\\\n",
    "             .merge(clustered_output[[col_id, col_cluster_id, 'Seed_Q1','Seed_Q2','Dist_to_Center','Cohesion']],\n",
    "                    on=col_id, how='left')\n",
    "\n",
    "    # 2. MERGE 'temp' WITH ORIGNAL DATA (without any drops) 'df'\n",
    "    #    --> maps cluster_info to orignal records using on='dup_similar_idx'\n",
    "    #\n",
    "    OUTPUT_DF = df.merge(temp[[col_dup_sim_id,col_cluster_id,'Seed_Q1', 'Seed_Q2','Dist_to_Center','Cohesion']],\n",
    "                         on=col_dup_sim_id, how='left').drop(columns=[col_dup_cluster_ids])\n",
    "\n",
    "    # 3. GENERATE INSIGHTS (Frequency, Coverage)\n",
    "    #\n",
    "    #  Frequency = how frequent is the Text_i across all Texts\n",
    "    #  Coverage  = % of frequency, i.e. Count of Text_i(including duplicates) / total count of all Texts\n",
    "    #\n",
    "    OUTPUT_DF[col_cluster_id] = OUTPUT_DF[col_cluster_id].fillna(-1).astype(int)\n",
    "    cid_count = Counter(OUTPUT_DF[col_cluster_id])\n",
    "    OUTPUT_DF[col_cluster_freq] = OUTPUT_DF[col_cluster_id].apply(lambda cid: cid_count.get(cid, 0))\n",
    "    OUTPUT_DF[col_cluster_coverage] = OUTPUT_DF[col_cluster_freq].apply(lambda x: int(x)*100.0/sum(cid_count.values()))\n",
    "    print(\"Total utterances:\", len(df))\n",
    "    print(\"Clusters generated:\", len(cid_count))\n",
    "    OUTPUT_DF.to_csv(\"\")\n",
    "    return OUTPUT_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "code_folding": [
     12
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total utterances: 500\n",
      "Clusters generated: 75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>Processed_tweet</th>\n",
       "      <th>lang_mask</th>\n",
       "      <th>dup_idx</th>\n",
       "      <th>similar_idx</th>\n",
       "      <th>dup_similar_idx</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>Seed_Q1</th>\n",
       "      <th>Seed_Q2</th>\n",
       "      <th>Dist_to_Center</th>\n",
       "      <th>Cohesion</th>\n",
       "      <th>memberCount</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uidx_0</td>\n",
       "      <td>All campus dining locations are closed today, ...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>campus dining location close today jan 1 happy...</td>\n",
       "      <td>en</td>\n",
       "      <td>['uidx_0']</td>\n",
       "      <td>['uidx_0']</td>\n",
       "      <td>('uidx_0',)</td>\n",
       "      <td>0</td>\n",
       "      <td>dining update seec cafe starbucks close begin ...</td>\n",
       "      <td>dining update begin today march 16 follow dini...</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.502</td>\n",
       "      <td>7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uidx_1</td>\n",
       "      <td>What are your campus dining options today, Jan...</td>\n",
       "      <td>cu_others</td>\n",
       "      <td>what your campus dining option today jan 2 alf...</td>\n",
       "      <td>en</td>\n",
       "      <td>['uidx_1']</td>\n",
       "      <td>['uidx_1', 'uidx_3']</td>\n",
       "      <td>('uidx_1', 'uidx_3')</td>\n",
       "      <td>1</td>\n",
       "      <td>dining update umc market starbucks alferd pack...</td>\n",
       "      <td>dining update alfred packer grill starbucks um...</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.550</td>\n",
       "      <td>7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uidx_2</td>\n",
       "      <td>#FPGA Design for #Embedded #Systems\\n\\n#SoC #V...</td>\n",
       "      <td>cu_online</td>\n",
       "      <td>fpga design embed system soc verilog vlsi asic...</td>\n",
       "      <td>en</td>\n",
       "      <td>['uidx_2']</td>\n",
       "      <td>['uidx_2']</td>\n",
       "      <td>('uidx_2',)</td>\n",
       "      <td>2</td>\n",
       "      <td>conjunction team visit samuel jackman prescod ...</td>\n",
       "      <td>buff take barbados team feature alongside thei...</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.492</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      UID                                              tweet      label  \\\n",
       "0  uidx_0  All campus dining locations are closed today, ...  cu_others   \n",
       "1  uidx_1  What are your campus dining options today, Jan...  cu_others   \n",
       "2  uidx_2  #FPGA Design for #Embedded #Systems\\n\\n#SoC #V...  cu_online   \n",
       "\n",
       "                                     Processed_tweet lang_mask     dup_idx  \\\n",
       "0  campus dining location close today jan 1 happy...        en  ['uidx_0']   \n",
       "1  what your campus dining option today jan 2 alf...        en  ['uidx_1']   \n",
       "2  fpga design embed system soc verilog vlsi asic...        en  ['uidx_2']   \n",
       "\n",
       "            similar_idx       dup_similar_idx  cluster_id  \\\n",
       "0            ['uidx_0']           ('uidx_0',)           0   \n",
       "1  ['uidx_1', 'uidx_3']  ('uidx_1', 'uidx_3')           1   \n",
       "2            ['uidx_2']           ('uidx_2',)           2   \n",
       "\n",
       "                                             Seed_Q1  \\\n",
       "0  dining update seec cafe starbucks close begin ...   \n",
       "1  dining update umc market starbucks alferd pack...   \n",
       "2  conjunction team visit samuel jackman prescod ...   \n",
       "\n",
       "                                             Seed_Q2  Dist_to_Center  \\\n",
       "0  dining update begin today march 16 follow dini...           0.531   \n",
       "1  dining update alfred packer grill starbucks um...           0.461   \n",
       "2  buff take barbados team feature alongside thei...           0.540   \n",
       "\n",
       "   Cohesion  memberCount  coverage  \n",
       "0     0.502            7       1.4  \n",
       "1     0.550            7       1.4  \n",
       "2     0.492            5       1.0  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = create_final_output(df, df_cluster, clustered_output)\n",
    "final_df.to_csv(os.path.join(data_dir, \"tweets_final_output.csv\"), index=False)\n",
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Metrics and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarized cluster Output showing N members in a list\n",
    "# DISPLAY_CLUSTER_MEMBER_COUNT = 30\n",
    "# visual = OUTPUT_DF.sort_values(by=[source_coverage_col, 'Dist_to_Center'], ascending=[False, True]).reset_index(drop=True)\n",
    "# visual_at_glance = visual.groupby([source_clustering_col]).agg({source_text_col: lambda x: list(x)[:DISPLAY_CLUSTER_MEMBER_COUNT], 'Seed_Q1': 'max', source_freq_col: 'max', source_coverage_col: 'max', 'Cohesion': 'max'}).reset_index()\n",
    "# visual_at_glance.rename(columns={source_text_col: f'List of {DISPLAY_CLUSTER_MEMBER_COUNT} {source_text_col}', 'Seed_Q1': \"Cluster Representative\"}, inplace=True)\n",
    "\n",
    "# # Top N Clusters\n",
    "# DISPLAY_N_CLUSTERS = 20\n",
    "# top_N_cluster_ids = list(visual[source_clustering_col].unique()[:DISPLAY_N_CLUSTERS])\n",
    "# top_N_clusters = visual_at_glance[visual_at_glance[source_clustering_col].isin(top_N_cluster_ids)].sort_values(by=[source_coverage_col], ascending=False).reset_index(drop=True)\n",
    "# top_N_clusters.rename(columns={source_text_col: f'List of {DISPLAY_CLUSTER_MEMBER_COUNT} {source_text_col}', 'Seed_Q1': \"Cluster Representative\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare Top N Clusters to be visualized!\n",
    "\n",
    "# top_N_clusters['DESC'] = top_N_clusters.apply(lambda row: \"Cluster Description:<br>Cluster ID = {}<br><br>Few Members = {}<br>Total Member count = {}<br>Coverage = {}<br>Cohesion/Tightness = {}\".format(row[source_clustering_col],\n",
    "#                                                                                                         row[f'List of {DISPLAY_CLUSTER_MEMBER_COUNT} {source_text_col}'][:5],\n",
    "#                                                                                                         row[source_freq_col],\n",
    "#                                                                                                         f'{row[source_coverage_col]:.3f}',\n",
    "#                                                                                                         f'{row[\"Cohesion\"]:.3f}'), axis=1)\n",
    "# import random\n",
    "# top_N_clusters['COLOR'] = top_N_clusters[source_clustering_col].apply(lambda x: \"#%02X%02X%02X\" % (random.randint(0,255), random.randint(0,255), random.randint(0,255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=go.Scattergl(\n",
    "\n",
    "#     x = top_N_clusters[source_coverage_col],\n",
    "#     y = top_N_clusters['Cohesion'],\n",
    "\n",
    "#     mode=\"markers\",\n",
    "#     marker=dict(\n",
    "#         size=top_N_clusters[source_coverage_col]/0.1,\n",
    "#         color=top_N_clusters['COLOR'],\n",
    "#         colorscale='Viridis',\n",
    "#         colorbar=dict(title='Cluster_score'),\n",
    "#         line_width=1,\n",
    "#         showscale=False),\n",
    "\n",
    "#     # Hover\n",
    "#     text = top_N_clusters[source_coverage_col],  # Mention Hover values here & formatting in hovertemplate\n",
    "#     hoverlabel=dict(bgcolor=\"black\", font_size=12, font_family=\"Roboto\"),\n",
    "#     hovertemplate=top_N_clusters['DESC']\n",
    "    \n",
    "# #     hovertemplate = \"<b>Cluster ID = %{text}</b><br>\" +\n",
    "# #                     \"<i>Few Members:</i>: %{members}<br>\" +\n",
    "# #                     \"<i>Cohesion</i>: %{y:.2f}<br>\" +\n",
    "# #                     \"<i>Cohesion</i>: %{y:.2f}<br>\" +\n",
    "# #                     \"<i>Coverage</i>: %{x:.2f}<br>\"\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(autosize=True,\n",
    "#                    title_text='Cluster map showing weighted-mean of Cohesion-Coverage values',\n",
    "#                    xaxis=dict(title=\"Coverage %\"),\n",
    "#                    yaxis=dict(title=\"Cohesion/Tightness\"),\n",
    "#                    showlegend=False) #width=900, height=700\n",
    "\n",
    "# fig.show()\n",
    "# # save\n",
    "# offline.plot(fig, filename=os.path.join(output_fp, \"Plot_CohesionVsCoverage%_for_topN_cluster.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.pie(top_N_clusters, values='COVERAGE', names='DESC')\n",
    "# fig.update_traces(textposition='outside')\n",
    "# fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
    "# fig.update_layout(autosize=True,\n",
    "#                   title_text=f'Relative Coverage % for top {DISPLAY_N_CLUSTERS} clusters',\n",
    "#                   xaxis=dict(title=\"Coverage %\"),\n",
    "#                   yaxis=dict(title=\"Cohesion/Tightness\"),\n",
    "#                   showlegend=False)\n",
    "# fig.show()\n",
    "# # save\n",
    "# offline.plot(fig, filename=os.path.join(output_fp, \"Relative_Coverage_%_for_topN_cluster.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.bar(top_N_clusters, x=top_N_clusters['Cluster_ID'].apply(lambda x: f\"Cluster_{x}\"), y='COVERAGE', color='COLOR', \n",
    "#              hover_data=dict(hovertemplate=top_N_clusters['DESC']))\n",
    "\n",
    "# fig.update_layout(title_text='Cluster map showing different clusters with their relative coverage %',\n",
    "#                   xaxis=dict(title=\"Cluster IDs\"),\n",
    "#                   yaxis=dict(title=\"Coverage %\"),\n",
    "#                   showlegend=False)\n",
    "\n",
    "# fig.show()\n",
    "# # save\n",
    "# offline.plot(fig, filename=os.path.join(output_fp, \"Coverage%_for_topN_cluster.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.8",
   "language": "python",
   "name": "python_3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
