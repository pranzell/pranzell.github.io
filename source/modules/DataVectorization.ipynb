{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vectorization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@author: Pranjal Pathak\n",
    "@date: 2023-02-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT TERMINOLOGIES\n",
    "\n",
    "#### IMPORTANT NOTE\n",
    "Throughout the notebook, the term \"tweet\" refers specifically to the context of the search terms that were utilized to query the API for data extraction. The context for these search terms, which now represent the tweets, as their `topic` or Labels, can be found in the `\"3. Raw Dataset\"` section.\n",
    "\n",
    "\n",
    "#### TWITTER\n",
    "\n",
    "\n",
    "- **Anchor**: The term used to tag a particular person, entity or another twitter handle. Such as \"@CUBoulder\" or \"@BoulderPolice\" are anchor terms used to tag University of Colorado Boulder twitter handle, and official Boulder County Police handle.\n",
    "\n",
    "\n",
    "- **Hastag**: The term used to highlight a common word or a phrase to make it detectable for seach engine crawlers, other Twitter users and could be associated with trending topics. Such as \"BlackLivesMatter\" or \"BLM\" is not an entity but a term which could be used by other Twitter account holders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Total Read Time ~ 20 mins.`\n",
    "\n",
    "`Total Execution Time ~ 45 mins.`\n",
    "\n",
    "#### EDA and Preprocessing\n",
    "1. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810282943eea4140b4a28a267ba9fdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n",
      "Spacy loaded.\n",
      "PyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "'''Python 3.8.0'''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "from ast import literal_eval\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "#from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/somepath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "#from spacy.lang.en import English\n",
    "from spacy.language import Language\n",
    "from spacy_language_detection import LanguageDetector\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as Functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt, ticker as ticker\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(\"../\")\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "models_dir = os.path.join(root_dir, \"models\")\n",
    "output_dir = os.path.join(root_dir, \"output\")\n",
    "\n",
    "sbert_model_fp = os.path.join(os.path.join(root_dir, \"models\"), \"transformer_models/all-distilroberta-v1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, \"processed_cuboulder_TwitterData.csv\"))\n",
    "df.hastags = df.hastags.apply(lambda x: literal_eval(x) if str(x) not in ['none', 'nan', 'np.nan', 'null', ''] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3394, 22)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     2,
     11,
     33,
     38
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer, model, max_length=128, embedding_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Mean Pooling - Take attention mask into account for correct averaging\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        encoded_input = tokenizer(text, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def transform(self, text: List[str], formatt='tensor'):\n",
    "        \"\"\" MODIFIED LATEST JAN 27, 2023\"\"\"\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "        \n",
    "        # default previously returned embeddings\n",
    "        embeddings = self._tokenize(text)\n",
    "        \n",
    "        # new **modified**\n",
    "        if formatt:\n",
    "            formatt = str(formatt).strip().lower()\n",
    "            if formatt=='tensor':\n",
    "                return embeddings\n",
    "            elif formatt=='numpy':\n",
    "                return embeddings.numpy()\n",
    "            elif formatt=='csr':\n",
    "                embeddings_matrix = Functional.normalize(embeddings, p=2, dim=1)\n",
    "                embeddings_matrix_csr = csr_matrix(embeddings_matrix.numpy().astype(np.float64))\n",
    "                return embeddings_matrix_csr\n",
    "            else:\n",
    "                raise Exception(\"Invalid input for formatt!\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting required so we just return ourselves. For fine-tuning, refer to shared gpu-code!\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Model '/Volumes/Local Drive/WORK/Machine Learning/0. Portfolio - ML Algorithms By Hand (Revised)/GITHUB/pranzell.github.io/source/models/transformer_models/all-distilroberta-v1/' loaded.\n"
     ]
    }
   ],
   "source": [
    "# SENTENCE-BERT VECTORIZATION #\n",
    "\n",
    "# load tokenizer, model classes\n",
    "tokenizer = AutoTokenizer.from_pretrained(sbert_model_fp)\n",
    "model_bert = AutoModel.from_pretrained(sbert_model_fp)\n",
    "\n",
    "# load vectorizer\n",
    "bert_vectorizer = BertTransformer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n",
    "print(\"Bert Model '%s' loaded.\" % sbert_model_fp)\n",
    "\n",
    "## SAMPLE FOR VECTORIZATION\n",
    "# corpus = df['text_col']\n",
    "# embeddings = bert_vectorizer.transform(corpus)\n",
    "# embeddings = bert_vectorizer.transform(corpus, formatt='numpy')\n",
    "# embeddings = bert_vectorizer.transform(corpus, formatt='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing\n",
    "\n",
    "embeddings = bert_vectorizer.transform(df['Processed_tweet'], formatt='numpy')\n",
    "embeddings.to_pickle(os.path.join(os.path.join(root_dir, \"models\"), \"processed_text_embeddings.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "Similarity_FastBert",
  "createdOn": 1648120119910,
  "creationTag": {
   "lastModifiedBy": {
    "login": "pranjp"
   },
   "lastModifiedOn": 1648120119910,
   "versionNumber": 0
  },
  "creator": "pranjp",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python_3.8",
   "language": "python",
   "name": "python_3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "modifiedBy": "pranjp",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
